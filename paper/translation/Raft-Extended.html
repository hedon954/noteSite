<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Raft-Extended 翻译 | Hedon</title>
    <meta name="generator" content="VuePress 1.8.2">
    
    <meta name="description" content="Just playing around">
    
    <link rel="preload" href="/noteSite/assets/css/0.styles.2b5d8ceb.css" as="style"><link rel="preload" href="/noteSite/assets/js/app.485e8ecd.js" as="script"><link rel="preload" href="/noteSite/assets/js/2.0f8877a4.js" as="script"><link rel="preload" href="/noteSite/assets/js/45.e1a2a372.js" as="script"><link rel="prefetch" href="/noteSite/assets/js/10.bad21fa2.js"><link rel="prefetch" href="/noteSite/assets/js/11.d69e71ed.js"><link rel="prefetch" href="/noteSite/assets/js/12.cecce23e.js"><link rel="prefetch" href="/noteSite/assets/js/13.4c4e4c0c.js"><link rel="prefetch" href="/noteSite/assets/js/14.09de4d83.js"><link rel="prefetch" href="/noteSite/assets/js/15.ff0d878a.js"><link rel="prefetch" href="/noteSite/assets/js/16.f2378c7f.js"><link rel="prefetch" href="/noteSite/assets/js/17.814aa3c9.js"><link rel="prefetch" href="/noteSite/assets/js/18.3974ce91.js"><link rel="prefetch" href="/noteSite/assets/js/19.aefa6ab7.js"><link rel="prefetch" href="/noteSite/assets/js/20.f836eb92.js"><link rel="prefetch" href="/noteSite/assets/js/21.370c2924.js"><link rel="prefetch" href="/noteSite/assets/js/22.facaa045.js"><link rel="prefetch" href="/noteSite/assets/js/23.b98bf01c.js"><link rel="prefetch" href="/noteSite/assets/js/24.3a321c23.js"><link rel="prefetch" href="/noteSite/assets/js/25.ea34d041.js"><link rel="prefetch" href="/noteSite/assets/js/26.5dfbcd09.js"><link rel="prefetch" href="/noteSite/assets/js/27.6e2cffc7.js"><link rel="prefetch" href="/noteSite/assets/js/28.7ab2d37a.js"><link rel="prefetch" href="/noteSite/assets/js/29.26a212e7.js"><link rel="prefetch" href="/noteSite/assets/js/3.04d298fd.js"><link rel="prefetch" href="/noteSite/assets/js/30.77dcb6bb.js"><link rel="prefetch" href="/noteSite/assets/js/31.9f27ae04.js"><link rel="prefetch" href="/noteSite/assets/js/32.c6d65303.js"><link rel="prefetch" href="/noteSite/assets/js/33.d74cb72c.js"><link rel="prefetch" href="/noteSite/assets/js/34.8644c67b.js"><link rel="prefetch" href="/noteSite/assets/js/35.8ebd8cd4.js"><link rel="prefetch" href="/noteSite/assets/js/36.8b3f7c5a.js"><link rel="prefetch" href="/noteSite/assets/js/37.812004f4.js"><link rel="prefetch" href="/noteSite/assets/js/38.336440f4.js"><link rel="prefetch" href="/noteSite/assets/js/39.00f92053.js"><link rel="prefetch" href="/noteSite/assets/js/4.80421812.js"><link rel="prefetch" href="/noteSite/assets/js/40.46ffd833.js"><link rel="prefetch" href="/noteSite/assets/js/41.0c1c8a20.js"><link rel="prefetch" href="/noteSite/assets/js/42.c10ccb89.js"><link rel="prefetch" href="/noteSite/assets/js/43.f8640aaa.js"><link rel="prefetch" href="/noteSite/assets/js/44.317ab46b.js"><link rel="prefetch" href="/noteSite/assets/js/5.4170108b.js"><link rel="prefetch" href="/noteSite/assets/js/6.f3d86aa7.js"><link rel="prefetch" href="/noteSite/assets/js/7.b86dc3e1.js"><link rel="prefetch" href="/noteSite/assets/js/8.2f2bf67e.js"><link rel="prefetch" href="/noteSite/assets/js/9.1df1d60f.js">
    <link rel="stylesheet" href="/noteSite/assets/css/0.styles.2b5d8ceb.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/noteSite/" class="home-link router-link-active"><img src="/noteSite/images/hedon.png" alt="Hedon" class="logo"> <span class="site-name can-hide">Hedon</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/noteSite/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/noteSite/guide/" class="nav-link">
  Guide
</a></div><div class="nav-item"><a href="/noteSite/java/" class="nav-link">
  Java
</a></div><div class="nav-item"><a href="/noteSite/golang/" class="nav-link">
  Golang
</a></div><div class="nav-item"><a href="/noteSite/linux/" class="nav-link">
  Linux
</a></div><div class="nav-item"><a href="/noteSite/mac/" class="nav-link">
  Mac
</a></div><div class="nav-item"><a href="/noteSite/paper/" class="nav-link router-link-active">
  Paper
</a></div><div class="nav-item"><a href="https://github.com/hedon954/noteSite" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/noteSite/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/noteSite/guide/" class="nav-link">
  Guide
</a></div><div class="nav-item"><a href="/noteSite/java/" class="nav-link">
  Java
</a></div><div class="nav-item"><a href="/noteSite/golang/" class="nav-link">
  Golang
</a></div><div class="nav-item"><a href="/noteSite/linux/" class="nav-link">
  Linux
</a></div><div class="nav-item"><a href="/noteSite/mac/" class="nav-link">
  Mac
</a></div><div class="nav-item"><a href="/noteSite/paper/" class="nav-link router-link-active">
  Paper
</a></div><div class="nav-item"><a href="https://github.com/hedon954/noteSite" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>翻译</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/noteSite/paper/translation/Raft-Extended.html" aria-current="page" class="active sidebar-link">Raft-Extended 翻译</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#辨析" class="sidebar-link">辨析</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_0-摘要" class="sidebar-link">0. 摘要</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_1-介绍" class="sidebar-link">1. 介绍</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_2-复制状态机" class="sidebar-link">2. 复制状态机</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_3-paxos-存在的问题" class="sidebar-link">3. Paxos 存在的问题</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_4-为可理解性而设计" class="sidebar-link">4. 为可理解性而设计</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_5-raft-共识算法" class="sidebar-link">5. Raft 共识算法</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_5-1-raft-基础" class="sidebar-link">5.1 Raft 基础</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_5-2-leader-election" class="sidebar-link">5.2 Leader election</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_5-3-log-replication" class="sidebar-link">5.3 Log replication</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_5-4-safety" class="sidebar-link">5.4 Safety</a></li></ul></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#参考" class="sidebar-link">参考</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="raft-extended-翻译"><a href="#raft-extended-翻译" class="header-anchor">#</a> Raft-Extended 翻译</h1> <blockquote><p>原文：https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf</p></blockquote> <h2 id="辨析"><a href="#辨析" class="header-anchor">#</a> 辨析</h2> <p><strong>consensus</strong> vs <strong>consistency</strong></p> <p>一致性（consistency）往往指分布式系统中多个副本对外呈现的数据的状态。如顺序一致性、线性一致性，描述了多个节点对数据状态的维护能力。</p> <p>共识（consensus）则描述了分布式系统中多个节点之间，彼此对某个提案达成一致结果的过程。</p> <p>因此，一致性描述的是<strong>结果</strong>，共识则是一种<strong>手段</strong>。</p> <p>有的人会说一致性和共识实际上是一个问题的一体两面，某种程度上来说，共识方法确实可以看作是实现强一致性的一种方法。事实上在工业界有许多以共识算法作为核心组件的多副本状态机（Replicated State Machine）实现，本质上利用了共识算法保证了所有副本的操作日志具有完全相同的顺序，从而实现了副本的一致性。但是，即使是在这样的场景下，讨论一个共识算法的一致性也是不合适的，因<strong>为整个分布式系统最终的一致性并不单单取决于共识算法，共识算法只是解决了其中一个问题。</strong></p> <blockquote><p>参考：https://zhuanlan.zhihu.com/p/68743917</p></blockquote> <h2 id="_0-摘要"><a href="#_0-摘要" class="header-anchor">#</a> 0. 摘要</h2> <p>Raft 是用来管理复制日志（replicated log）的一致性协议。它跟 multi-Paxos 作用相同，效率也相当。但是它的组织结构跟 Paxos 不同，也是因为 Raft 更简单的架构使得它更容易被理解，并且更容易在实际工程中得以实现。</p> <p>为了让 Raft 更容易被理解，Raft 将共识算法的关键性因素切分成几个部分，比如：</p> <ul><li>leader election（领导者选举）</li> <li>log replication（日志复制）</li> <li>safety（安全性）</li></ul> <p>并且 Raft 实施了一种更强的共识性以便减少必须要考虑的状态（states）的数量。</p> <p>用户研究表明，对于学生来说，Raft 相比于 Paxos 是更容易学习的。</p> <p>Raft 还包括一个用于解决<strong>变更集群成员问题</strong>的新机制，它使用重写多数来保证安全性。</p> <h2 id="_1-介绍"><a href="#_1-介绍" class="header-anchor">#</a> 1. 介绍</h2> <p>共识算法允许多台机器作为一个集群协同工作，并且在其中的某几台机器出故障时集群仍然能正常工作。正因为如此，共识算法在建立可靠的大规模软件系统方面发挥了重要作用。在过去十年中，Paxos [15,16] 主导了关于共识算法的讨论：大多数共识性的实现都是基于 Paxos 或受其影响，Paxos 已经成为教授学生关于共识知识的主要工具。</p> <p>比较遗憾的是，尽管很多人一直在努力尝试使 Paxos 更易懂，Paxos 还是太难理解了。此外，Paxos 的架构需要复杂的改变来支持实际系统。这导致的结果就是系统开发者和学生在学生和使用 Paxos 过程中都很挣扎。</p> <p>在我们自己与 Paxos 斗争之后，我们开始着手寻找一个新的共识算法，希望可以为系统开发和教学提供更好的基础。 我们的方法是不寻常的，因为我们的主要目标是可理解性：我们可以设计一个比 Paxos 更适合用于实际工程实现并且更易懂的共识算法吗？</p> <p>在该算法的设计中，重要的不仅是如何让算法起作用，还要清晰地知道该算法为什么会起作用。</p> <p>这项工作的结果是一个称为 Raft 的共识性算法。在设计 Raft 时，我们使用了特定的技术来提高它的可理解性，包括：</p> <ul><li>分解（Raft 分离出三个关键点：leader election、log replication、safety）</li> <li>减少状态空间（相比于 Paxos，Raft 降低了不确定性的程度和服务器之间的不一致）</li></ul> <p>一项针对 2 所大学共 43 名学生的用户研究表明，Raft 比 Paxos 更容易理解：在学习两种算法后，其中 33 名学生能够更好地回答 Raft 的相关问题。</p> <p>Raft 在许多方面类似于现有的公式算法（尤其是 Oki、Liskov 的 Viewstamped Replication [29,22]），但它有几个新特性：</p> <ul><li><strong>Strong leader（强领导性）</strong>：相比于其他算法，Raft 使用了更强的领导形式。比如，日志条目只能从 leader 流向 follower（集群中除 leader 外其他的服务器）。这在使 Raft 更易懂的同时简化了日志复制的管理流程。</li> <li><strong>Leader election（领导选举）</strong>：Raft 使用随机计时器来进行领导选举。任何共识算法都需要心跳机制（heartbeats），Raft 只需要在这个基础上，添加少量机制，就可以简单快速地解决冲突。</li> <li><strong>Membership changes（成员变更）</strong>：Raft 在更改集群中服务器集的机制中使用了一个**联合共识（joint consensus）**的方法。在联合共识（joint consensus）下，在集群配置的转换过程中，新旧两种配置大多数是重叠的，这使得集群在配置更改期间可以继续正常运行。</li></ul> <p>我们认为 Raft 跟 Paxos 以及其他共识算法相比是更优的，这不仅体现在教学方面，还体现在工程实现方面。</p> <ul><li>它比其他算法更简单且更易于理解</li> <li>它被描述得十分详细足以满足实际系统的需要</li> <li>它有多个开源实现，并被多家公司使用</li> <li>它的安全性已被正式规定和验证</li> <li>它的效率与其他算法相当</li></ul> <p>本文剩余部分：</p> <table><thead><tr><th>所在节</th> <th>内容</th></tr></thead> <tbody><tr><td>第 2 节</td> <td>复制状态机问题（replicated state machine problem）</td></tr> <tr><td>第 3 节</td> <td>Paxos 的优缺点</td></tr> <tr><td>第 4 节</td> <td>实现 Raft 易理解性的措施</td></tr> <tr><td>第 5-8 节</td> <td>Raft 共识性算法详细阐述</td></tr> <tr><td>第 9 节</td> <td>评估 Raft</td></tr> <tr><td>第 10 节</td> <td>其他相关工作</td></tr></tbody></table> <h2 id="_2-复制状态机"><a href="#_2-复制状态机" class="header-anchor">#</a> 2. 复制状态机</h2> <p>共识算法一般都是在复制状态机 [37] 的背景下实现的。在这种方法下，一组服务器在的状态机计算相同状态的相同副本，即使某些服务器宕机，它们也可以继续运行。</p> <p>复制状态机是用来解决分布式系统中的各种容错问题。比如说，具有单个 leader 的大规模的系统，如 GFS [8]，HDFS [38] 和 RAMCloud [33] ，他们通常都使用单独的复制状态机来管理 leader election 和保存 leader 崩溃后重新选举所需的配置信息。像 Chubby [2] 和 ZooKeeper [11] 都是复制状态机。</p> <p>复制状态机通常都是使用日志复制（log replication）来实现。如图1：每个服务器都保存着一份拥有一系列命令的日志，然后服务器上的状态机会按顺序执行日志中的命令。每一份日志中命令相同并且顺序也相同，因此每个状态机可以处理相同的命令序列。所以状态机是可确定的，每个状态机都执行相同的状态和相同的输出序列。</p> <p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gs8ll0hozyj323i0mg7aw.jpg" alt="image-20210707191314206"></p> <p>共识算法的主要工作就是保证复制日志（replicated log）的一致性。每台服务器上的共识模块接收来自客户端的命令，并将这些命令添加到其日志当中。它（指共识模块）与其他服务器上的共识模块进行通信，以确保每台服务器上最终以相同的顺序包含相同的命令，即使部分服务器宕机了，这个条件也可以满足。一旦命令被正确复制，每台服务器上的状态机就会按日志顺序处理它们，并将输出返回给客户端。这样就形成了高可用的复制状态机。</p> <p>适用于实际系统的共识算法通常都包含以下几点特征：</p> <ul><li><p>它们确保在所有非拜占庭错误下的安全性，也就是从不返回一个错误的结果。（即使是网络延迟、分区、数据包丢失、数据包重复和数据包乱序）</p> <blockquote><p><strong><a href="https://zh.wikipedia.org/wiki/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E9%97%AE%E9%A2%98" target="_blank" rel="noopener noreferrer">拜占庭错误<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>：</strong></p> <p>出现故障（crash 或 fail-stop，即不响应）但不会伪造信息的情况称为“非拜占庭错误”。</p> <p>伪造信息恶意响应的情况称为“拜占庭错误”，对应节点称为拜占庭节点。</p></blockquote></li> <li><p>只要任何大多数（过半）服务器是可运行的，并且可以互相通信和与客户端通信，那么共识算法就可用。假设服务器宕机了，一小段时间后，它们很可能会根据已经稳定存储的状态来进行恢复，并重新加入集群。</p></li> <li><p>它们在保证日志一致性上不依赖于时序：错误的时钟和极端消息延迟在最坏的情况下会产生影响可用性的一系列问题。</p></li> <li><p>在通常情况下，只要集群中大部分（过半）服务器已经响应了单轮远程过程调用（RPC），命令就可以被视为完成。少数（一半以下）慢服务器不会影响整个系统的性能。</p></li></ul> <h2 id="_3-paxos-存在的问题"><a href="#_3-paxos-存在的问题" class="header-anchor">#</a> 3. Paxos 存在的问题</h2> <p>在过去的十年间，Leslie Lamport 的 Paxos 协议 [15] 几乎成为共识性（consensus）的同义词。它是课堂上被教授最多的共识协议，大多数共识性的实现也是以它为起点。Paxos 首先定义了能在单个决策问题（例如单个复制日志条目）上达成共识的协议。我们将这个子集称为 <em>signle-degree Paxos</em>。然后 Paxos 组合该协议的多个实例去实现一系列决策，比如日志（<em>mutil-Paxos</em>）。Paxos 保证了安全性和活性，它也支持改变集群中的成员，它的安全性也已经被论证了，并且大多数情况下都是高效的。</p> <p>美中不足的是，Paxos 有两个严重的缺点：</p> <ol><li><p><strong>Paxos 非常难理解</strong></p> <p>众所周知，Paxos 非常晦涩难懂，除非下了很大的功夫，很少有人能够成功理解它。因此，尽管目前已经有几个尝试希望将 Paxos [16,20,21]  解释得通俗易懂一些，而且这些解释都集中在 <code>single-decree Paxos</code>，但是它们还是很难懂。</p> <p>在对 NSDI 2012 参会者的非正式调查中，我们发现很少人会喜欢 Paxos，即使是经验丰富的研究人员。我们自己也一直在跟 Paxos 作斗争，我们也无法完全理解整个  Paxos 协议，直到阅读了几个更简单的描述和自己设计了替代 Paxos 的协议，我们才对 Paxos 有了比较深刻的理解。但这个过程，花了将近一年。</p> <p>我们推测 Paxos 这么晦涩难懂，主要是因为作者选择了 <code>Single-decree Paxos</code> 来作为基础。<code>Single-decree Paxso</code> 非常搞人：它分为两个阶段，但是并没有对这两个阶段进行简单直观的说明，而且这两个阶段也不能分开了单独理解，所以使用者将就很难理解为什么该算法能起作用。<code>Multi-Paxos</code> 的合成规则又增加了许多复杂性。我们相信，对多个决定（日志，并非单个日志条目）达成共识的总体问题可以用其他更直接和更明显的方式进行分解。</p></li> <li><p><strong>Paxos 没有为实际实现提供一个良好的基础</strong></p> <p>其中一个原因是没有广泛认同的针对 <code>Multi-Paxos</code> 的算法。Lamport 的描述主要是针对 <code>signle-decree Paxos</code> 的，他描述了针对 <code>multi-Paxos</code> 的可能方法，但缺少了很多细节。</p> <p>目前已经有人在尝试具体化和优化 Paxos，比如 [26]，[39] 和 [13]，但是这些尝试都互不相同并且它们跟 Lamport 描述的也不尽相同。虽然像 Chubby [4] 这样的系统已经实现了类 Paxos（Paxos-like）算法，但是他们并没有透露出很多的实现细节。</p></li></ol> <p>此外，Paxos 的架构对于构建实际系统来说其实是一个糟糕的设计，这是 <code>single-decree Paxos</code> 分解的另一个结果。举个例子，这对于独立选择地日志条目的集合，然后再将它们合并到顺序日志当中没有任何好处，这只会增加复杂性。围绕日志来设计系统是更加简单和高效的方法，其中新条目按受约束的顺序依次附加。另外一个问题是 Paxos 在其核心使用了<strong>对称对等方法</strong>（尽管它最终表明了这会被用作一种性能优化的弱领导模式）。这在只有一个决策的情况下是有意义的，但是尽管如此，还是很少有实际系统采用了这种方法。如果有一系列的决策需要制定，更简单和更快速的方法应该是首先选择一个 leader，然后由 leader 去协调这些决策。</p> <p>因此，按照 Paxos 来实现的实际系统往往跟 Paxos 相差很大。几乎所有的实现都是从 Paxos 开始，然后在实现的过程中发现了一系列的难题，在解决难题的过程中，开发出了跟 Paxos 完全不一样的架构。这样既费时又容易出错，而且 Paxos 本身的晦涩难懂又使得问题变得更加严重。Paxos 公式可能是证明其正确性的一个很好的公式，但真正的实现与 Paxos 又相差很大，这证明了它其实没有什么价值。下面来自 Chubby 作者的评论非常典型：</p> <blockquote><p>在 Paxos 算法描述和现实实现系统之间有着巨大的鸿沟... （如果一直按照 Paxos 算法走下去），最终的系统往往会建立在一个还未被证明的协议之上。</p></blockquote> <p>综合上述问题，我们觉得 Paxos 在教学端和系统构建端都没有提供一个良好的基础。考虑到共识性在大规模软件系统中的重要性，我们决定去尝试一下看看能不能设计一个替代 Paxos 并且具有更好特性的共识算法。Raft 就是这次实验的结果。</p> <h2 id="_4-为可理解性而设计"><a href="#_4-为可理解性而设计" class="header-anchor">#</a> 4. 为可理解性而设计</h2> <p>在设计 Raft 算法过程中我们有几个目标：</p> <ul><li>它必须为系统构建提供一个完整且实际的基础，这样才能大大减少开发者的工作</li> <li>它必须在任何情况下都是安全的并且在典型的应用条件下是可用的，并且在正常情况下是高效的</li></ul> <p>但是我们最重要的目标，也是我们遇到的最大的挑战：</p> <ul><li>它必须具有易理解性，它必须保证能够被大多数人轻松地理解。而且它必须能够让人形成直观的认识，这样系统构建者才能在实现过程中对它进行不可避免的拓展。</li></ul> <p>在设计 Raft 算法的过程中，很多情况下我们需要在多个备选方案下做出抉择。在这种情况下，我们往往会基于可理解性来进行抉择：</p> <ul><li>解释各个备选方案的难度有多大？例如，它的状态空间有多复杂？它是否具有难以理解的含义？</li> <li>对于一个读者来说，完成理解这个方案和方案中的各种含义是否简单？</li></ul> <p>我们意识到这一的分析具有高度的主观性。所以我们采取了两种通用的措施来解决这个问题。</p> <p>第一个措施就是众所周知的问题分解：只要有可能，我们就将问题划分成几个相对独立地解决、解释和理解的子问题。例如，Raft 算法被我们划分成 leader 选举、日志复制、安全性和成员变更几个部分。</p> <p>第二个措施是通过减少状态的数量来简化状态空间，尽可能地使系统变得更加连贯和尽可能地消除不确定性。很明显的一个例子就是，所有的日志都是不允许有空挡的，并且 Raft 限制了日志之间可能不一样的方式。尽管在大多数情况下我们都极力去消除不确定性，但是在某些情况下不确定性却可以提高可理解性。一个重要的例子就是随机化方法，它们虽然引入了不确定性，但是它们往往能够通过以类似的方式处理所有可能的选择来减少状态空间（随便选，没关系）。所有我们使用了随机化来简化 Raft 中的 leader election 算法。</p> <h2 id="_5-raft-共识算法"><a href="#_5-raft-共识算法" class="header-anchor">#</a> 5. Raft 共识算法</h2> <p>Raft 是一种用来管理第 2 节中提到的复制日志（replicated log）的算法。图 2 是该算法的浓缩，可以作为参考。图 3 列举了该算法的一些关键特性。这两张图中的内容将会在后面的各个章节中逐一介绍。</p> <p>Raft 在实现共识算法的过程中，首先选举一个 distinguished leader，然后由该 leader 全权负责复制日志的一致性。Leader 从客户端接收日志条目，然后将这些日志条目复制给其他服务器，并且在保证安全性的情况下通知其他服务器将日志条目应用到他们的状态机中。拥有一个 leader 大大简化了对复制日志的管理流程。例如，leader 可以在不跟其他服务器商议的情况下决定新的日志条目应该存放在日志的什么位置，并且数据都是从 leader 流向其他服务器。当然了，一个 leader 可能会宕机，也可能与其他服务器断开连接，那么这个时候，Raft 就会选举出一个新的 leader 出来。</p> <p>通过选举一个 leader 的方式，Raft 将共识问题分解成三个独立的子问题，这些问题将会在接下来的子章节中进行讨论：</p> <ul><li><p><strong>Leader election（领导选举）</strong></p> <p>一个 leader 倒下之后，一定会有一个新的 leader 站起来。</p></li> <li><p><strong>Log replication（日志复制）</strong></p> <p>leader 必须接收来自客户端的日志条目然后复制到集群中的其他节点，并且强制其他节点的日志和自己的保持一致。</p></li> <li><p><strong>Safety（安全性）</strong></p> <p>Raft 中安全性的关键是图 3 中状态机的安全性：只要有任何服务器节点将一个特定的日志条目应用到它的状态机中，那么其他服务器节点就不能在同一个日志索引位置上存储另外一条不同的指令。第 5.4 节将会描述 Raft 如何保证这种特性，而且该解决方案在 5.2 节描述的选举机制上还增加了额外的限制。</p></li></ul> <p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gsar1v5rklj32300pc4bj.jpg" alt="image-20210709155333989"></p> <p>在展示了 Raft 共识算法后，本章节将讨论可用性的一些问题以及时序在系统中的所用。</p> <h3 id="_5-1-raft-基础"><a href="#_5-1-raft-基础" class="header-anchor">#</a> 5.1 Raft 基础</h3> <p>一个 Raft 集群中包含若干个服务器节点，<font color="green"><strong>5 个一个比较典型的数字，5 个服务器的集群可以容忍 2 个节点的失效</strong></font>。在任何一个时刻，集群中的每一个节点都只可能是以下是三种身份之一：</p> <ul><li>leader：它会处理所有来自客户端的请求（如果一个客户端和 follower 通信，follower 会将请求重定向到 leader 上）</li> <li>follower：它们被动的：它们不会发送任何请求，只是简单的响应来自 leader 和 candidate 的请求</li> <li>candidate：这是用来选举一个新的 leader 的时候出现的一种临时状态，这将在第 5.2 节中详细描述</li></ul> <p>在正常情况下，集群中只有一个 leader，然后剩下的节点都是 follower。图 4 展示了这些状态和它们之间的转换关系，这些转换关系将会在接下来进行讨论。</p> <p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gsap8d6ijjj322s0l47g5.jpg" alt="image-20210709145034498"></p> <p>如图 5 所示，Raft 将时间划分成任意长度的任期（term）。每一段任期从一次选举开始，在这个时候会有一个或者多个 candidate 尝试去成为 leader。如果某一个 candidate 赢得了选举，那么它就会在任期剩下的时间里承担一个 leader 的角色。在某些情况下，一次选举无法选出 leader，这个时候这个任期会以没有 leader 而结束。同时一个新的任期（包含一次新的选举）会很快重新开始。这是因为 Raft 会保证在任意一个任期内，至多有一个 leader。</p> <p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gsapcmhhi4j31ym0ig78s.jpg" alt="image-20210709145441879"></p> <p>集群中不同的服务器观察到的任期转换的次数也许是不同的，在某些情况下，一个节点可能没有观察到 leader 选举过程甚至是整个任期过程。</p> <p>任期在 Raft 中还扮演着一个逻辑时钟（logical clock）的角色，这使得服务器可以发现一些过期的信息，比如过时的 leader。</p> <p>每一个节点都存储着一个当前任期号（current term number），该任期号会随着时间<strong>单调递增</strong>。节点之间通信的时候会交换当前任期号，如果一个节点的当前任期号比其他节点小，那么它就将自己的任期号更新为较大的那个值。如果一个 candidate 或者 leader 发现自己的任期号过期了，它就会立刻回到 follower 状态。如果一个节点接收了一个带着过期的任期号的请求，那么它会拒绝这次请求。</p> <p>Raft 算法中服务器节点之间采用 RPC 进行通信，一般的共识算法都只需要两种类型的 RPC。</p> <ul><li><strong>RequestVote RPCs（请求投票）</strong>：由 candidate 在选举过程中发出（5.2 节中描述）</li> <li><strong>AppendEntries RPCs（追加条目）</strong>：由 leader 发出，用来做日志复制和提供心跳机制（5.3 节中描述）。</li></ul> <p>在第 7 节中为了在节点之间传输快照（snapshot）增加了第三种 RPC。当节点没有及时的收到 RPC 的响应时，会进行重试，而且节点之间都是以并行（parallel）的方式发送 RPC 请求，以此来获得最佳的性能。</p> <h3 id="_5-2-leader-election"><a href="#_5-2-leader-election" class="header-anchor">#</a> 5.2 Leader election</h3> <p>Raft 采用一种心跳机制来触发 leader 选举。当服务器启动的时候，他们都会称为 follower。一个服务器节点只要从 candidate 或者 leader 那接收到有效的 RPC 就一直保持 follower 的状态。Leader 会周期性地向所有的 follower 发起心跳来维持自己的 leader 地位，所谓心跳，就是不包含日志条目的 AppendEntries RPC。如果一个 follower 在一段时间内没有收到任何信息（这段时间我们称为<strong>选举超时 election timeout</strong>），那么它就会假定目前集群中没有一个可用的 leader，然后开启一次选举来选择一个新的 leader。</p> <p>开始进行选举的时候，一个 follower 会自增当前任期号然后切换为 candidate 状态。然后它会给自己投票，同时以并行的方式发送一个 RequestVote RPCs 给集群中的其他服务器节点（企图得到它们的投票）。一个 candidate 会一直保持当前状态直到以下的三件事之一发生（这些情况都会在下面的章节里分别讨论）：</p> <ul><li>它赢得选举，成为了 leader</li> <li>其他节点赢得了选择，那么它会变成 follower</li> <li>一段时间之后没有任何节点在选举中胜出</li></ul> <p>当一个 candidate 获取集群中过半服务器节点针对同一任期的投票时，它就赢得了这次选举并成为新的 leader。对于同一个任期，每一个服务器节点会按照**先来先服务原则（first-come-first-served）**只投给一个 candidate（在5.4 节会在投票上增加额外的限制）。这种要求获得过半投票才能成为 leader 的规则确保了最多只有一个 candidate 赢得此次选举（图 3 中的选举安全性）。只要有一个 candidate 赢得选举，它就会成为 leader。然后它就会向集群中其他节点发送心跳消息来确定自己的地位并阻止新的选举。</p> <p>一个 candidate 在等待其他节点给它投票的时候，它也有可能接收到另外一个自称为 leader 的节点给它发过来的 AppendEntries RPC。</p> <ul><li>如果这个 leader 的任期号（这个任期号会在这次 RPC 中携带着）不小于这个 candidate 的当前任期号，那么这个 candidate 就会觉得这个 leader 是合法的，然后将自己转变为 follower 状态。</li> <li>如果这个 leader 的任期号小于这个 candidate 的当前热七号，那么这个 candidate 就会拒绝这次 RPC，然后继续保持 candidate 状态。</li></ul> <p>第三种可能的结果是 candidate 既没有赢得选举也没有输。可以设想一下这么一个长裤。所有的 follower 同时变成  candidate，然后它们都将票投给自己，那这样就没有 candidate 能得到超过半数的投票了，投票无果。当这种情况发生的时候，每个 candidate 都会进行一次超时响应（time out），然后通过自增任期号来开启一轮新的选举，并启动另一轮的 RequestVote RPCs。然而，如果没有额外的措施，这种无结果的投票可能会无限重复下去。</p> <p>为了解决上述问题，Raft 采用**随机选举超时时间（randomized election timeouts）**来确保很少发生无果的投票，并且就算发生了也能很快地解决。<strong>为了防止选票一开始就被瓜分，选举超时时间是从一个固定的区间（比如，150-300ms）中随机选择。这样可以把服务器分散开来以确保在大多数情况下会只有一个服务器率先结束超时，那么这个时候，它就可以赢得选举并在其他服务器结束超时之前发送心跳</strong>（译者注：乘虚而入，不讲武德）。</p> <p>同样的机制也可以被用来解决选票被瓜分（split votes）的情况。每个 candidate 在开始一轮选举之前会重置一个随机选举超时时间，然后一直等待直到结束超时状态。这样减少了在一次投票无果后再一次投票无果的可能性。9.3 节展示了该方案能够快速地选出一个 leader。</p> <p>选举的例子可以很好地展现可理解性是如何指导我们在多种备选设计方案中做出抉择的。在一开始，我们本打算使用一种等级系统（rank system）：每一个 candidate 被赋予一个一次的等级（rank），如果一个 candidate 发现另外一个 candidate 有着更高的登记，那么它就会返回 follower 状态，这样可以使高等级的 candidate 更加容易地赢得下一轮选举。但是我们发现这种方法在可用性方面会有一些小问题：**如果等级较高的服务器宕机了，那么等级较低的服务器可能需要进入超时状态，然后重新成为一个 candidate。如果这种操作出现得太快，那么它可能会重启进程去开启一轮新的选举。**经过我们对该算法做出了多次的调整，我们最终还是认为随机重试的方法更加通俗易懂。</p> <h3 id="_5-3-log-replication"><a href="#_5-3-log-replication" class="header-anchor">#</a> 5.3 Log replication</h3> <p>Leader 一旦被选举出来，它就要开始为客户端的请求提供服务了。每一个客户端请求都包含一条将被复制状态机执行的命令。leader 会以一个新条目的方式将该命令追加到自己的日志中，并且以同步的方式向集群中的其他节点发起 AppendEntires RPCs，让它们复制该条目。当条目被安全地复制（何为安全复制，后面会介绍）之后，leader 会将该条目应用到自己的状态机中，状态机执行该指令，然后把执行的结果返回给客户端。如果 follower 宕机了或者运行缓慢，或者网络丢包，leader 会不断地重试 AppendEntiries RPCs（即使已经对客户端作出了响应）直到所有的 follower 都成功存储了所有的日志条目。</p> <p>日志以图 6 展示的方式组织着。每条日志条目都存储着一条状态机指令和 leader 收到该指定时的任期号。日志条目中的任期号可以用来检测多个日志副本之间是否不一致，以此来保证图 3 中的某些性质。每个日志条目还有一个整数索引值来表明它在日志中的位置。</p> <p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gsasp7ss8gj32220ssjy9.jpg" alt="image-20210709165036190"></p> <p>那么问题就来了，**leader 什么时候会觉得把日志条目应用到状态机是安全的呢？**这种日志条目被称为已提交的日志条目。Raft 保证这种已提交的日志条目都是持久化的并且最终都会被所有可用的状态机执行。**一旦创建该日志条目的 leader 将它复制到过半的节点上时（比如图 6 中的条目 7），该日志条目就会被提交。**同时，leader 日志中该日志条目之前的所有日志条目也都会被提交，包括由之前的其他 leader 创建的日志条目。5.4 节会讨论在 leader 变更之后应用该规则的一些细节，并证明这种提交的规则是安全的。leader 会追踪它所知道的要提交的最高索引，并将该索引包含在未来的 AppendEntries RPC 中（包括心跳），以便其他的节点可以发现这个索引。一旦一个 follower 知道了一个日志条目被提交了。它就会将该日志条目按日志顺序应用到自己的状态机中。</p> <p>我们设计 Raft 日志机制来使得不同节点上的日志之间可以保持高水平的一致性。这么做不仅简化了系统的行为也使得系统更加可预测，同时该机制也是保证安全性的重要组成部分。Raft 会一直维护着以下的特性，这些特性也同时构成了图 3 中的日志匹配特性（Log Matching Property）：</p> <ul><li>如果不同日志中的两个条目有着相同的索引和任期值，那么它们就存储着相同的命令</li> <li>如果不同日志中的两个条目有着相同的索引和任期值，那么他们之前的所有日志条目也都相同</li></ul> <p>第一条特性源于这样一个事实，在给定的一个任期值中，一个 leader 最多创建一个带有给定日志索引的条目，而且日志条目永远不会改变它们在日志中的位置。</p> <p>第二条特性是由 AppendEntries RPC 执行的一个简单的一致性检查所保证的。当 leader 发送一个 AppendEntries RPC 的时候，leader 会将前一个日志条目的索引位置和任期号包含在里面（紧邻最新的日志条目）。如果一个 follower 在它的日志中找不到包含相同索引位置和任期号的条目，那么它就会拒绝该新的日志条目。一致性检查就像一个归纳步骤：一开始空的日志状态肯定是满足日志匹配特性（Log Matching Property）的，然后一致性检查保证了日志扩展时的日志匹配特性。因此，当 AppendEntries RPC 返回成功时，leader 就知道 follower 的日志一定和自己相同（从第一个日志条目到最新条目）。</p> <p>正常操作期间，leader 和 follower 的日志都是保持一致的，所以 AppendEntries 的一致性检查从来不会失败。但是，如果 leader 崩溃了，那么就有可能会造成日志处于不一致的状态，比如说老的 leader 可能还没有完全复制它日志中的所有条目它就宕机了。这些不一致的情况会在一系列的 leader 和 follower 崩溃的情况下加剧。图 7 解释了什么情况下 follower 的日志可能和新的 leader 的日志不同。follower 可能会确实一些在新 leader 中有的日志条目，也有可能拥有一些新的 leader 没有的日志条目，或者同时存在。缺失或多出日志条目的情况有可能会涉及到多个任期。</p> <p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gse5vxj5sfj31se0u017c.jpg" alt="image-20210712144330139"></p> <p>在 Raft 算法中，leader 通过强制 follower 复制 leader 日志来解决日志不一致的问题。也就是说，follower 中跟 leader 冲突的日志条目会被 leader 的日志条目所覆盖。5.4 节会证明通过增加一个限制，这种方式就可以保证安全性。</p> <p>为了使 follower 的日志跟自己（leader）一致，leader 必须找到两者达成一致的最大的日志条目索引，删除 follower 日志中从那个索引之后的所有日志条目，并且将自己那个索引之后的所有日志条目发送给 follower。所有的这些操作都发生在 AppendEntries RPCs 的一致性检查的回复中。leader 维护着一个针对每一个 follower 的 <strong>nextIndex</strong>，这个 nextIndex 代表的就是 leader 要发送给 follower 的下一个日志条目的索引。<strong>当选出一个新的 leader 时，该 leader 将所有的 nextIndex 的值都初始化为自己最后一个日志条目的 index 加 1（图7 中的 11）</strong>。如果一个 follower 的日志跟 leader 的是不一致的，那么下一次的 AppendEntries RPC 的一致性检查就会失败。<strong>AppendEntries RPC 在被 follower 拒绝之后，leader 对 nextIndex 进行减 1，然后重试 AppendEntries RPC。最终 nextIndex 会在某个位置满足 leader 和 follower 在该位置及之前的日志是一致的，此时，AppendEntries RPC 就会成功，将 follower 跟 leader 冲突的日志条目全部删除然后追加 leader 中的日志条目（需要的话）</strong>。一旦 AppendEntries RPC 成功，follower 的日志就和 leader 的一致了，并且在该任期接下来的时间里都保持一致。</p> <blockquote><p>如果需要的话，下面的协议可以用来优化被拒绝的 AppendEntries RPCs 的个数。</p> <p>比如说，当拒绝一个 AppendEntries RPC 的时候，follower 可以包含冲突条目的任期号和自己存储的那个任期的第一个 index。借助这些信息，leader 可以跳过那个任期内所有的日志条目来减少 indexIndex。这样就变成了每个有冲突日志条目的任期只需要一个 AppendEntries RPC，而不是每一个日志条目都需要一次 AppendEntires RPC。</p> <p>在实践中，我们认为这种优化是没有必要的，因为失败不经常发生并且也不可能有很多不一致的日志条目。</p></blockquote> <p>通过上述机制，leader 在当权之后就不需要任何特殊的操作来使日志恢复到一致状态。leader 只需进行正常的操作，然后日志就能在回复  AppendEntries RPC 一致性检查的时候自动趋于一致。leader 从来不会重写或者删除自己的日志条目（图3 中的 Leader Append-Only 属性）。</p> <p>上述这种日志复制机制展现了第 2 节中描述的 Raft 算法的共识特性：只要过半的节点能正常运行，Raft 就能接受、复制并处理新的日志条目。在通常情况下，一个新的条目可以在一轮 RPC 中被复制给集群中过半的节点，并且单个运行缓慢的 follower 并不会影响整个集群的性能。</p> <h3 id="_5-4-safety"><a href="#_5-4-safety" class="header-anchor">#</a> 5.4 Safety</h3> <p>前面的章节描述了 Raft 如何做 Leader Election 和 Log Replication。然而，到目前为止所讨论的机制并不能充分地保证每一个状态机会按相同的顺序执行相同的指令。比如说，一个 follower 可能会进入不可用状态，在此期间，leader 可能提交了若干的日志条目，然后这个 follower 可能被选举为新的 leader 并且用新的日志条目去覆盖这些日志条目。这样就会造成不同的状态机执行不同的指令的情况。</p> <p>本节通过对 Leader Election 增加一个限制来完善 Raft 算法。这个限制保证了对于给定的任意任期号，该任期号对应的 leader 都包含了之前各个任期所有被提交的日志条目（图3 中的 Leader Completeness 性质）。有了这个限制，我们也可以使日志提交规则更加清晰。最后，我们会展示对于 Leader Completeness 性质的简要证明并且说该性质是如何保证状态机执行正确的行为的。</p> <h4 id="_5-4-1-选举限制"><a href="#_5-4-1-选举限制" class="header-anchor">#</a> 5.4.1 选举限制</h4> <p>在任何基于 leader 的共识算法中，leader 最终都必须存储所有已经提交的日志条目。在某些共识算法中，例如 Viewstamped Replication [22]，即使一个节点它一开始并没有包含所有已经提交的日志条目，它也有可能被选举为 leader。这些算法包含一些额外的机制来识别丢失的日志条目并将它们传送给新的 leader，这个机制要么发生在选举阶段，要么在选举完成之后很快进行。比较遗憾的是，这种方法会增加许多额外的机制，使得算法复杂性大大增加。Raft 使用了一种更加简单的方法，它可以保证新 leader 在当选时就包含了之前所有任期中已经提交的日志条目，根本就不需要再传送这些日志条目给新的 leader。这就意味着<strong>日志条目的传送只有一个方向，那就是从 leader 到 follower，leader 从来不会覆盖本地日志中已有的日志。</strong></p> <p>Raft 采用投票的方式来保证一个 candidate 只有拥有之前所有任期中已经提交的日志条目之后，才有可能赢得选举。一个 candidate 如果想要被选为 leader，那它就必须跟集群中超过半数的节点进行通信，这就意味这些节点中至少一个包含了所有已经提交的日志条目。如果 candidate 的日志至少跟过半的服务器节点一样新，那么它就一定包含了所有以及提交的日志条目，一旦有投票者自己的日志比 candidate 的海信，那么这个投票者就会拒绝该投票，该 candidate 也就不会赢得选举。</p> <blockquote><p>所谓“<strong>新</strong>”：</p> <p>Raft 通过比较两份日志中的最后一条日志条目的索引和任期号来定义谁的日志更新。</p> <ul><li>如果两份日志最后条目的任期号不同，那么任期号大的日志更新</li> <li>如果两份日志最后条目的任期号相同，那么谁的日志更长，谁就更新</li></ul></blockquote> <h4 id="_5-4-2-提交之前任期内的日志条目"><a href="#_5-4-2-提交之前任期内的日志条目" class="header-anchor">#</a> 5.4.2 提交之前任期内的日志条目</h4> <blockquote><p>译者注：注意！这一节「提交之前任期内的日志条目」这种操作 Raft 的不允许的！本小节只是用来举一种错误情况！</p></blockquote> <p>如 5.3 节中提到的那样，一旦当前任期内的某个日志条目以及存储到过半的服务器节点上，leader 就知道该日志可以被提交了。如果这个 leader 在提交某个日志条目之前崩溃了，以后的 leader 会尝试完成该日志条目的复制。然而，如果是之前任期内的某个日志条目已经存储到了过半的服务器节点上了，新任期内的 leader 也无法立即断定该日志条目已经被提交了。图 8 展示了一种情况：一个已经被存储到过半节点的老日志条目，仍然有可能会被未来的 leader 覆盖掉。</p> <p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gse88t4iicj31wc0u0ts1.jpg" alt="image-20210712160506841"></p> <blockquote><p>译者注：<strong>对图 8 的理解的补充</strong>。</p> <p><font color="orange">参考：</font></p> <ul><li><a href="https://zhuanlan.zhihu.com/p/369989974" target="_blank" rel="noopener noreferrer">知乎<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <p><font color="orange">核心：</font></p> <ul><li><strong>图 8 用来说明为什么 leader 不能提交之前任期的日志，只能通过提交自己任期的日志，从而间接提交之前任期的日志。</strong></li></ul> <p><font color="orange">分析：</font></p> <ol><li>先按错误的情况，也就是 leader 提交之前任期的日志，那么上述的流程：
<ol><li>(a) S1 是任期 2 的 leader，日志已经复制给了 S2，此时还没过半；</li> <li>(b) S1 宕机，S5 获得了 S3、S4、S5 的投票成为  leader，然后写了一个日志条目（index=2，term=3）；</li> <li>(c) S5 刚写完日志，还没来得及复制，就宕机了，此时 S1 和 S2 都可能当选，加入 S1 当选（currentTerm=4），此刻还没有新的请求进来，S1 将日志条目（index=2，term=2）复制给了 S3，多数派达成，S1 提交了这个日志条目（index=2，term=2），**注意，该日志不是当前任期内的日志，我们在讨论错误的情况！ **然后请求进来，S1 写日志条目（index=3，term=4），然后 S1 宕机。</li> <li>情况一：(d) S5 重启，因为 S5 最后的日志条目的任期号比 S2、S3 大，所以 S5 可以赢得选举（currentTerm=5），S5 将日志条目（index=2，item=3）复制给其他所有节点并提交，**此时 index=2 的日志条目被提交了两次！一次 term=2，一次term=3，这是不被允许的，因为已经提交的日志条目是不能被覆盖的！**✖️</li> <li>情况二：(e) S1 在宕机之前将自己的日志条目（index=3，term=4）复制到了过半节点上，这种情况下，S5 不可能选举成功。这是 S1 不发生故障，这是正确复制的情况。✔️</li></ol></li></ol> <p>所以**「leader 可以提交之前任期的日志」<strong>这种操作是不允许的，我们需要加上约束：</strong>「leader 只能提交自己任期的日志」**。</p> <ol start="2"><li><p>加了约束之后，前面的 (a) 和 (b) 没有改变，从 (c) 开始：</p> <ol><li><p>(c) S1 还是将日志条目（index=2，term=2）复制给其他节点，它复制给了 S3，此时已经复制给了过半的节点了，但是<strong>由于 currentTerm=4，所以 S1 还是不能提交该日志条目</strong>。如果 S1 将日志条目（index=3，term=4）也复制给了过半的节点，S1 是可以提交该日志条目的，那么这个时候，前面的日志条目（index=2，term=2）也会被间接提交，这就是 (e) 所展示的情况。</p></li> <li><p>(d) S1 还是将日志条目（index=2，term=2）复制给其他节点，它复制给了 S3，此时已经复制给了过半的节点了，但是<strong>由于 currentTerm=4，所以 S1 还是不能提交该日志条目</strong>。但是这个时候，S1 只是日志条目（index=3，term=4）写入自己的日志，还没来得及复制就宕机了。然后 S5 重启并赢得了选举（currentTerm=5），然后将日志条目（index=2，term=3）复制给其他所有节点，现在 index=2 的日志条目是没有提交过的，S5 能提交该日志吗？</p> <p><strong>不能！因为 leader 不能提交之前任期的日志！只有等新的请求进来，超过半数节点复制了 1-3-5 之后，term=3 的日志才能跟着 term=5 的日志一起被提交。</strong></p></li></ol></li></ol> <p><font color="orange">延伸：</font></p> <p>加了上述约束后，就不会出现同一个 index 上的日志条目被重复提交的情况了，但是这又多出了另外一个问题了：<strong>如果一直没有新的请求进来，那么日志条目（index=2，term=3）岂不是就一直不能提交？那不就阻塞了吗？</strong></p> <p>这里如果是 kv 数据库，问题就很明显了。假设 (c) 或 (d) 中的日志条目（index=2）里的 Command 是 <code>Set(&quot;k&quot;, &quot;1&quot;)</code>，S5 当选 leader 后，客户端来查询 <code>Get(&quot;k&quot;)</code>，leader 查到日志有记录但又不能回复 1 给客户端（因为按照约束这条日志未提交），线性一致性要求不能返回陈旧的数据，leader 迫切地需要知道这条日志到底能不能提交。</p> <p>所以 Raft 论文提高了引入 <strong>no-op 日志</strong>来解决这个问题，这个在 etcd 中有实现。</p> <p><font color="orange">no-op 日志：</font></p> <p>no-op 日志即只有 index 和 term 信息，command 信息为空。也是要写到磁盘存储的。</p> <p>具体流程是<strong>在 leader 刚选举成功的时候，立即追加一条 no-op 日志，并立即复制到其它节点，no-op 日志一经提交，leader 前面那些未提交的日志全部间接提交，问题就解决了。像上面的 kv 数据库，有了 no-op 日志之后，Leader 就能快速响应客户端查询了。</strong></p> <p>本质上，no-op 日志使 leader 隐式地快速提交之前任期未提交的日志，确认当前 <code>commitIndex</code>，这样系统才会快速对外正常工作。</p></blockquote> <p>为了解决图 8 中描述的问题，Raft 永远不会通过计算副本数目的方式来提交之前任期内的日志条目。只有 leader 当期内的日志条目才通过计算副本数目的方式来提交。一旦当前任期内的某个日志条目以这种方式被提交（如图 8 中的 e），那么由于日志匹配特性（Log Matching），之前的所有日志条目也会被间接地提交。在某些情况下，leader 可以安全地断定一个老的日志条目已经被提交（例如，如果该条目已经被存储到每一个节点上了）。但是 Raft 为了简化问题，采取了上述描述的更加保守的方法。</p> <p>Raft 会在提交规则上增加额外的复杂性是因为当 leader 复制之前任期内的日志条目时，这些日志条目都保留原来的任期号。在其他的共识算法中，如果一个新的 leader 要重新复制之前任期里的日志时，它必须使用当前新的任期号。Raft 的做法使得更加容易推导出日志条目，因为它们自始至终都使用同一个任期号。另外，和其他的算法相比，Raft 中的新 leader 只需要发送更少的日志条目（其他算法中必须在它们被提交之前发送更多的冗余日志条目来给它们重新编号）。</p> <h4 id="_5-4-3-安全性论证"><a href="#_5-4-3-安全性论证" class="header-anchor">#</a> 5.4.3 安全性论证</h4> <p>给出了完整的 Raft 算法后，我们现在可以更严格地来论证 leader 完整性特性（Leader Completeness Property）（这一讨论基于 9.2 节的安全性证明）。我们先假设 Leader Completeness Property 是不满足的，然后再推出矛盾来。</p> <p><strong>假设：</strong></p> <p>假设任期 T 的 leader~T~ 在任期内提交了一个日志条目，但是该日志条目没有存在未来某些任期的 leader 中，假设 U 是大于 T 的没有存储该日志条目的最小任期号，处在任期 U 的 leader 称为 leader~U~。</p> <p><strong>论证：</strong></p> <ol><li></li></ol> <h2 id="参考"><a href="#参考" class="header-anchor">#</a> 参考</h2> <p>[1]  BOLOSKY, W. J., BRADSHAW, D., HAAGENS, R. B., KUSTERS, N. P., AND LI, P. Paxos replicated state machines as the basis of a high-performance data store. In <em>Proc. NSDI’11, USENIX Conference on Networked Systems Design and Implementation</em> (2011), USENIX, pp. 141–154.</p> <p>[2]  BURROWS, M. The Chubby lock service for loosely- coupled distributed systems. In <em>Proc. OSDI’06, Sympo- sium on Operating Systems Design and Implementation</em> (2006), USENIX, pp. 335–350.</p> <p>[3]  CAMARGOS, L. J., SCHMIDT, R. M., AND PEDONE, F. Multicoordinated Paxos. In <em>Proc. PODC’07, ACM Sym- posium on Principles of Distributed Computing</em> (2007), ACM, pp. 316–317.</p> <p>[4]  CHANDRA, T. D., GRIESEMER, R., AND REDSTONE, J. Paxos made live: an engineering perspective. In <em>Proc. PODC’07, ACM Symposium on Principles of Distributed Computing</em> (2007), ACM, pp. 398–407.</p> <p>[5]  CHANG, F., DEAN, J., GHEMAWAT, S., HSIEH, W. C., WALLACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., AND GRUBER, R. E. Bigtable: a distributed storage system for structured data. In <em>Proc. OSDI’06, USENIX Symposium on Operating Systems Design and Implementation</em> (2006), USENIX, pp. 205–218.</p> <p>[6]  CORBETT, J. C., DEAN, J., EPSTEIN, M., FIKES, A., FROST, C., FURMAN, J. J., GHEMAWAT, S., GUBAREV, A., HEISER, C., HOCHSCHILD, P., HSIEH, W., KAN- THAK, S., KOGAN, E., LI, H., LLOYD, A., MELNIK, S., MWAURA, D., NAGLE, D., QUINLAN, S., RAO, R., ROLIG, L., SAITO, Y., SZYMANIAK, M., TAYLOR, C., WANG, R., AND WOODFORD, D. Spanner: Google’s globally-distributed database. In <em>Proc. OSDI’12, USENIX Conference on Operating Systems Design and Implemen- tation</em> (2012), USENIX, pp. 251–264.</p> <p>[7]  COUSINEAU, D., DOLIGEZ, D., LAMPORT, L., MERZ, S., RICKETTS, D., AND VANZETTO, H. TLA+ proofs. In <em>Proc. FM’12, Symposium on Formal Methods</em> (2012), D. Giannakopoulou and D. Me ́ry, Eds., vol. 7436 of <em>Lec- ture Notes in Computer Science</em>, Springer, pp. 147–154.</p> <p>[8]  GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In <em>Proc. SOSP’03, ACM Symposium on Operating Systems Principles</em> (2003), ACM, pp. 29–43.</p> <p>[9]  GRAY,C.,ANDCHERITON,D.Leases:Anefficientfault- tolerant mechanism for distributed file cache consistency. In <em>Proceedings of the 12th ACM Ssymposium on Operating Systems Principles</em> (1989), pp. 202–210.</p> <p>[10]  HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for concurrent objects. <em>ACM Trans- actions on Programming Languages and Systems 12</em> (July 1990), 463–492.</p> <p>[11]  HUNT, P., KONAR, M., JUNQUEIRA, F. P., AND REED, B . ZooKeeper: wait-free coordination for internet-scale systems. In <em>Proc ATC’10, USENIX Annual Technical Con- ference</em> (2010), USENIX, pp. 145–158.</p> <p>[12]  JUNQUEIRA, F. P., REED, B. C., AND SERAFINI, M. Zab: High-performance broadcast for primary-backup sys- tems. In <em>Proc. DSN’11, IEEE/IFIP Int’l Conf. on Depend- able Systems &amp; Networks</em> (2011), IEEE Computer Society, pp. 245–256.</p> <p>[13]  KIRSCH, J., AND AMIR, Y. Paxos for system builders. Tech. Rep. CNDS-2008-2, Johns Hopkins University, 2008.</p> <p>[14]  L A M P O RT, L . Time, clocks, and the ordering of events in a distributed system. <em>Commununications of the ACM 21</em>, 7 (July 1978), 558–565.</p> <p>[15]  L A M P O RT, L . The part-time parliament. <em>ACM Transac- tions on Computer Systems 16</em>, 2 (May 1998), 133–169.</p> <p>[16]  LAMPORT, L. Paxos made simple. <em>ACM SIGACT News 32</em>, 4 (Dec. 2001), 18–25.</p> <p>[17]  L A M P O RT, L . <em>Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers</em>. Addison- Wesley, 2002.</p> <p>[18]  LAMPORT, L. Generalized consensus and Paxos. Tech. Rep. MSR-TR-2005-33, Microsoft Research, 2005.</p> <p>[19] L A M P O RT, L . Fast paxos. (2006), 79–103.</p> <p>[20]  LAMPSON, B. W. How to build a highly available system using consensus. In <em>Distributed Algorithms</em>, O. Baboaglu and K. Marzullo, Eds. Springer-Verlag, 1996, pp. 1–17.</p> <p>[21]  LAMPSON, B. W. The ABCD’s of Paxos. In <em>Proc. PODC’01, ACM Symposium on Principles of Distributed Computing</em> (2001), ACM, pp. 13–13.</p> <p>[22]  LISKOV, B., AND COWLING, J. Viewstamped replica- tion revisited. Tech. Rep. MIT-CSAIL-TR-2012-021, MIT, July 2012.</p> <p>[23]  LogCabin source code. http://github.com/ logcabin/logcabin.</p> <p>[24]  LORCH, J. R., ADYA, A., BOLOSKY, W. J., CHAIKEN, R., DOUCEUR, J. R., AND HOWELL, J. The SMART way to migrate replicated stateful services. In <em>Proc. Eu- roSys’06, ACM SIGOPS/EuroSys European Conference on Computer Systems</em> (2006), ACM, pp. 103–115.</p> <p>[25]  MAO, Y., JUNQUEIRA, F. P., AND MARZULLO, K. Mencius: building efficient replicated state machines for WANs. In <em>Proc. OSDI’08, USENIX Conference on Operating Systems Design and Implementation</em> (2008), USENIX, pp. 369–384.</p> <p>[26] MAZIE` RES, D. Paxos made practical. http://www.scs.stanford.edu/ ̃dm/home/ papers/paxos.pdf, Jan. 2007.</p> <p>[27]  MORARU, I., ANDERSEN, D. G., AND KAMINSKY, M. There is more consensus in egalitarian parliaments. In <em>Proc. SOSP’13, ACM Symposium on Operating System Principles</em> (2013), ACM.</p> <p>[28]  Raft user study. http://ramcloud.stanford. edu/ ̃ongaro/userstudy/.</p> <p>[29]  OKI, B. M., AND LISKOV, B. H. Viewstamped replication: A new primary copy method to support highly-available distributed systems. In <em>Proc. PODC’88, ACM Symposium on Principles of Distributed Computing</em> (1988), ACM, pp. 8–17.</p> <p>[30]  O’NEIL, P., CHENG, E., GAWLICK, D., AND ONEIL, E. The log-structured merge-tree (LSM-tree). <em>Acta Informat- ica 33</em>, 4 (1996), 351–385.</p> <p>[31]  ONGARO, D. <em>Consensus: Bridging Theory and Practice</em>. PhD thesis, Stanford University, 2014 (work in progress).</p> <p>[32]  ONGARO, D., AND OUSTERHOUT, J. In search of an understandable consensus algorithm. In <em>Proc ATC’14, USENIX Annual Technical Conference</em> (2014), USENIX.</p> <p>[33]  OUSTERHOUT, J., AGRAWAL, P., ERICKSON, D., KOZYRAKIS, C., LEVERICH, J., MAZIE`RES, D., MI- TRA, S., NARAYANAN, A., ONGARO, D., PARULKAR, G., ROSENBLUM, M., RUMBLE, S. M., STRATMANN, E., AND STUTSMAN, R. The case for RAMCloud. <em>Com- munications of the ACM 54</em> (July 2011), 121–130.</p> <p>[34]  Raft consensus algorithm website. http://raftconsensus.github.io.</p> <p>[35]  REED, B. Personal communications, May 17, 2013.</p> <p>[36]  ROSENBLUM, M., AND OUSTERHOUT, J. K. The design and implementation of a log-structured file system. <em>ACM Trans. Comput. Syst. 10</em> (February 1992), 26–52.</p> <p>[37]  S C H N E I D E R , F. B . Implementing fault-tolerant services using the state machine approach: a tutorial. <em>ACM Com- puting Surveys 22</em>, 4 (Dec. 1990), 299–319.</p> <p>[38]  SHVACHKO, K., KUANG, H., RADIA, S., AND CHANSLER, R. The Hadoop distributed file system. In <em>Proc. MSST’10, Symposium on Mass Storage Sys- tems and Technologies</em> (2010), IEEE Computer Society, pp. 1–10.</p> <p>[39]  VAN RENESSE, R. Paxos made moderately complex. Tech. rep., Cornell University, 2012.</p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">7/12/2021, 8:45:08 PM</span></div></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/noteSite/assets/js/app.485e8ecd.js" defer></script><script src="/noteSite/assets/js/2.0f8877a4.js" defer></script><script src="/noteSite/assets/js/45.e1a2a372.js" defer></script>
  </body>
</html>
