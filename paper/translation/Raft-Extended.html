<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Raft-Extended 翻译 | Hedon</title>
    <meta name="generator" content="VuePress 1.8.2">
    
    <meta name="description" content="Just playing around">
    
    <link rel="preload" href="/noteSite/assets/css/0.styles.0f3b0cbd.css" as="style"><link rel="preload" href="/noteSite/assets/js/app.ee9f4817.js" as="script"><link rel="preload" href="/noteSite/assets/js/2.0f8877a4.js" as="script"><link rel="preload" href="/noteSite/assets/js/44.c1e4736b.js" as="script"><link rel="prefetch" href="/noteSite/assets/js/10.03168d59.js"><link rel="prefetch" href="/noteSite/assets/js/11.f8ea08ba.js"><link rel="prefetch" href="/noteSite/assets/js/12.cecce23e.js"><link rel="prefetch" href="/noteSite/assets/js/13.4c4e4c0c.js"><link rel="prefetch" href="/noteSite/assets/js/14.269eeaf8.js"><link rel="prefetch" href="/noteSite/assets/js/15.19638ff1.js"><link rel="prefetch" href="/noteSite/assets/js/16.a39942b3.js"><link rel="prefetch" href="/noteSite/assets/js/17.9900b78b.js"><link rel="prefetch" href="/noteSite/assets/js/18.f59dd181.js"><link rel="prefetch" href="/noteSite/assets/js/19.dedfd422.js"><link rel="prefetch" href="/noteSite/assets/js/20.b79d4824.js"><link rel="prefetch" href="/noteSite/assets/js/21.8735c004.js"><link rel="prefetch" href="/noteSite/assets/js/22.03b558e4.js"><link rel="prefetch" href="/noteSite/assets/js/23.44115a9e.js"><link rel="prefetch" href="/noteSite/assets/js/24.24dfd05b.js"><link rel="prefetch" href="/noteSite/assets/js/25.bfb911c7.js"><link rel="prefetch" href="/noteSite/assets/js/26.7ee73687.js"><link rel="prefetch" href="/noteSite/assets/js/27.fa63f2e9.js"><link rel="prefetch" href="/noteSite/assets/js/28.7ab2d37a.js"><link rel="prefetch" href="/noteSite/assets/js/29.c3dab096.js"><link rel="prefetch" href="/noteSite/assets/js/3.748dd9db.js"><link rel="prefetch" href="/noteSite/assets/js/30.77dcb6bb.js"><link rel="prefetch" href="/noteSite/assets/js/31.0d1c28b8.js"><link rel="prefetch" href="/noteSite/assets/js/32.dc09f388.js"><link rel="prefetch" href="/noteSite/assets/js/33.a23ddc3d.js"><link rel="prefetch" href="/noteSite/assets/js/34.8644c67b.js"><link rel="prefetch" href="/noteSite/assets/js/35.8ebd8cd4.js"><link rel="prefetch" href="/noteSite/assets/js/36.11fc2fec.js"><link rel="prefetch" href="/noteSite/assets/js/37.900cf285.js"><link rel="prefetch" href="/noteSite/assets/js/38.aa9f5bcf.js"><link rel="prefetch" href="/noteSite/assets/js/39.784866eb.js"><link rel="prefetch" href="/noteSite/assets/js/4.a61e4cea.js"><link rel="prefetch" href="/noteSite/assets/js/40.2bffac04.js"><link rel="prefetch" href="/noteSite/assets/js/41.c565a323.js"><link rel="prefetch" href="/noteSite/assets/js/42.a7f5f048.js"><link rel="prefetch" href="/noteSite/assets/js/43.614db209.js"><link rel="prefetch" href="/noteSite/assets/js/5.2bccd6df.js"><link rel="prefetch" href="/noteSite/assets/js/6.f3d86aa7.js"><link rel="prefetch" href="/noteSite/assets/js/7.80f82a7b.js"><link rel="prefetch" href="/noteSite/assets/js/8.f6285dc4.js"><link rel="prefetch" href="/noteSite/assets/js/9.945333ff.js">
    <link rel="stylesheet" href="/noteSite/assets/css/0.styles.0f3b0cbd.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/noteSite/" class="home-link router-link-active"><img src="/noteSite/images/hedon.png" alt="Hedon" class="logo"> <span class="site-name can-hide">Hedon</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/noteSite/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/noteSite/guide/" class="nav-link">
  Guide
</a></div><div class="nav-item"><a href="/noteSite/java/" class="nav-link">
  Java
</a></div><div class="nav-item"><a href="/noteSite/golang/" class="nav-link">
  Golang
</a></div><div class="nav-item"><a href="/noteSite/linux/" class="nav-link">
  Linux
</a></div><div class="nav-item"><a href="/noteSite/mac/" class="nav-link">
  Mac
</a></div><div class="nav-item"><a href="/noteSite/paper/" class="nav-link router-link-active">
  Paper
</a></div><div class="nav-item"><a href="https://github.com/hedon954/noteSite" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/noteSite/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/noteSite/guide/" class="nav-link">
  Guide
</a></div><div class="nav-item"><a href="/noteSite/java/" class="nav-link">
  Java
</a></div><div class="nav-item"><a href="/noteSite/golang/" class="nav-link">
  Golang
</a></div><div class="nav-item"><a href="/noteSite/linux/" class="nav-link">
  Linux
</a></div><div class="nav-item"><a href="/noteSite/mac/" class="nav-link">
  Mac
</a></div><div class="nav-item"><a href="/noteSite/paper/" class="nav-link router-link-active">
  Paper
</a></div><div class="nav-item"><a href="https://github.com/hedon954/noteSite" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>翻译</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/noteSite/paper/translation/Raft-Extended.html" aria-current="page" class="active sidebar-link">Raft-Extended 翻译</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#辨析" class="sidebar-link">辨析</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_0-摘要" class="sidebar-link">0. 摘要</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_1-介绍" class="sidebar-link">1. 介绍</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_2-复制状态机" class="sidebar-link">2. 复制状态机</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_3-paxos-存在的问题" class="sidebar-link">3. Paxos 存在的问题</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_4-为可理解性而设计" class="sidebar-link">4. 为可理解性而设计</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_5-raft-共识算法" class="sidebar-link">5. Raft 共识算法</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_5-1-raft-基础" class="sidebar-link">5.1 Raft 基础</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_5-2-leader-election" class="sidebar-link">5.2 Leader election</a></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#_5-3-log-replication" class="sidebar-link">5.3 Log replication</a></li></ul></li><li class="sidebar-sub-header"><a href="/noteSite/paper/translation/Raft-Extended.html#参考" class="sidebar-link">参考</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="raft-extended-翻译"><a href="#raft-extended-翻译" class="header-anchor">#</a> Raft-Extended 翻译</h1> <blockquote><p>原文：https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf</p></blockquote> <h2 id="辨析"><a href="#辨析" class="header-anchor">#</a> 辨析</h2> <p><strong>consensus</strong> vs <strong>consistency</strong></p> <p>一致性（consistency）往往指分布式系统中多个副本对外呈现的数据的状态。如顺序一致性、线性一致性，描述了多个节点对数据状态的维护能力。</p> <p>共识（consensus）则描述了分布式系统中多个节点之间，彼此对某个提案达成一致结果的过程。</p> <p>因此，一致性描述的是<strong>结果</strong>，共识则是一种<strong>手段</strong>。</p> <p>有的人会说一致性和共识实际上是一个问题的一体两面，某种程度上来说，共识方法确实可以看作是实现强一致性的一种方法。事实上在工业界有许多以共识算法作为核心组件的多副本状态机（Replicated State Machine）实现，本质上利用了共识算法保证了所有副本的操作日志具有完全相同的顺序，从而实现了副本的一致性。但是，即使是在这样的场景下，讨论一个共识算法的一致性也是不合适的，因<strong>为整个分布式系统最终的一致性并不单单取决于共识算法，共识算法只是解决了其中一个问题。</strong></p> <blockquote><p>参考：https://zhuanlan.zhihu.com/p/68743917</p></blockquote> <h2 id="_0-摘要"><a href="#_0-摘要" class="header-anchor">#</a> 0. 摘要</h2> <p>Raft 是用来管理复制日志（replicated log）的一致性协议。它跟 multi-Paxos 作用相同，效率也相当。但是它的组织结构跟 Paxos 不同，也是因为 Raft 更简单的架构使得它更容易被理解，并且更容易在实际工程中得以实现。</p> <p>为了让 Raft 更容易被理解，Raft 将共识算法的关键性因素切分成几个部分，比如：</p> <ul><li>leader election（领导者选举）</li> <li>log replication（日志复制）</li> <li>safety（安全性）</li></ul> <p>并且 Raft 实施了一种更强的共识性以便减少必须要考虑的状态（states）的数量。</p> <p>用户研究表明，对于学生来说，Raft 相比于 Paxos 是更容易学习的。</p> <p>Raft 还包括一个用于解决<strong>变更集群成员问题</strong>的新机制，它适用重写多数来保证安全性。</p> <h2 id="_1-介绍"><a href="#_1-介绍" class="header-anchor">#</a> 1. 介绍</h2> <p>共识算法允许多台机器作为一个集群协同工作，并且在其中的某几台机器出故障时集群仍然能正常工作。正因为如此，共识算法在建立可靠的大规模软件系统方面发挥了重要作用。在过去十年中，Paxos [15,16] 主导了关于共识算法的讨论：大多数共识性的实现都是基于 Paxos 或受其影响，Paxos 已经成为教授学生关于共识知识的主要工具。</p> <p>比较遗憾的是，尽管很多人一直在努力尝试使 Paxos 更易懂，Paxos 还是太难理解了。此外，Paxos 的架构需要复杂的改变来支持实际系统。这导致的结果就是系统开发者和学生在学生和使用 Paxos 过程中都很挣扎。</p> <p>在我们自己与 Paxos 斗争之后，我们开始着手寻找一个新的共识算法，希望可以为系统开发和教学提供更好的基础。 我们的方法是不寻常的，因为我们的主要目标是可理解性：我们可以设计一个比 Paxos 更适合用于实际工程实现并且更易懂的共识算法吗？</p> <p>在该算法的设计中，重要的不仅是如何让算法起作用，还要清晰地知道该算法为什么会起作用。</p> <p>这项工作的结果是一个称为 Raft 的共识性算法。在设计 Raft 时，我们使用了特定的技术来提高它的可理解性，包括：</p> <ul><li>分解（Raft 分离出三个关键点：leader election、log replication、safety）</li> <li>减少状态空间（相比于 Paxos，Raft 降低了不确定性的程度和服务器之间的不一致）</li></ul> <p>一项针对 2 所大学共 43 名学生的用户研究表明，Raft 比 Paxos 更容易理解：在学习两种算法后，其中 33 名学生能够更好地回答 Raft 的相关问题。</p> <p>Raft 在许多方面类似于现有的公式算法（尤其是 Oki、Liskov 的 Viewstamped Replication [29,22]），但它有几个新特性：</p> <ul><li><strong>Strong leader（强领导性）</strong>：相比于其他算法，Raft 使用了更强的领导形式。比如，日志条目只能从 leader 流向 follower（集群中除 leader 外其他的服务器）。这在使 Raft 更易懂的同时简化了日志复制的管理流程。</li> <li><strong>Leader election（领导选举）</strong>：Raft 使用随机计时器来进行领导选举。任何共识算法都需要心跳机制（heartbeats），Raft 只需要在这个基础上，添加少量机制，就可以简单快速地解决冲突。</li> <li><strong>Membership changes（成员变更）</strong>：Raft 在更改集群中服务器集的机制中使用了一个**联合共识（joint consensus）**的方法。在联合共识（joint consensus）下，在集群配置的转换过程中，新旧两种配置大多数是重叠的，这使得集群在配置更改期间可以继续正常运行。</li></ul> <p>我们认为 Raft 跟 Paxos 以及其他共识算法相比是更优的，这不仅体现在教学方面，还体现在工程实现方面。</p> <ul><li>它比其他算法更简单且更易于理解</li> <li>它被描述得十分详细足以满足实际系统的需要</li> <li>它有多个开源实现，并被多家公司使用</li> <li>它的安全性已被正式规定和验证</li> <li>它的效率与其他算法相当</li></ul> <p>本文剩余部分：</p> <table><thead><tr><th>所在节</th> <th>内容</th></tr></thead> <tbody><tr><td>第 2 节</td> <td>复制状态机问题（replicated state machine problem）</td></tr> <tr><td>第 3 节</td> <td>Paxos 的优缺点</td></tr> <tr><td>第 4 节</td> <td>实现 Raft 易理解性的措施</td></tr> <tr><td>第 5-8 节</td> <td>Raft 共识性算法详细阐述</td></tr> <tr><td>第 9 节</td> <td>评估 Raft</td></tr> <tr><td>第 10 节</td> <td>其他相关工作</td></tr></tbody></table> <h2 id="_2-复制状态机"><a href="#_2-复制状态机" class="header-anchor">#</a> 2. 复制状态机</h2> <p>共识算法一般都是在复制状态机 [37] 的背景下实现的。在这种方法下，一组服务器在的状态机计算相同状态的相同副本，即使某些服务器宕机，它们也可以继续运行。</p> <p>复制状态机是用来解决分布式系统中的各种容错问题。比如说，具有单个 leader 的大规模的系统，如 GFS [8]，HDFS [38] 和 RAMCloud [33] ，他们通常都使用单独的复制状态机来管理 leader election 和保存 leader 崩溃后重新选举所需的配置信息。像 Chubby [2] 和 ZooKeeper [11] 都是复制状态机。</p> <p>复制状态机通常都是使用日志复制（log replication）来实现。如图1：每个服务器都保存着一份拥有一系列命令的日志，然后服务器上的状态机会按顺序执行日志中的命令。每一份日志中命令相同并且顺序也相同，因此每个状态机可以处理相同的命令序列。所以状态机是可确定的，每个状态机都执行相同的状态和相同的输出序列。</p> <p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gs8ll0hozyj323i0mg7aw.jpg" alt="image-20210707191314206"></p> <p>共识算法的主要工作就是保证复制日志（replicated log）的一致性。每台服务器上的共识模块接收来自客户端的命令，并将这些命令添加到其日志当中。它（指共识模块）与其他服务器上的共识模块进行通信，以确保每台服务器上最终以相同的顺序包含相同的命令，即使部分服务器宕机了，这个条件也可以满足。一旦命令被正确复制，每台服务器上的状态机就会按日志顺序处理它们，并将输出返回给客户端。这样就形成了高可用的复制状态机。</p> <p>适用于实际系统的共识算法通常都包含以下几点特征：</p> <ul><li><p>它们确保在所有非拜占庭错误下的安全性，也就是从不返回一个错误的结果。（即使是网络延迟、分区、数据包丢失、数据包重复和数据包乱序）</p> <blockquote><p><strong><a href="https://zh.wikipedia.org/wiki/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E9%97%AE%E9%A2%98" target="_blank" rel="noopener noreferrer">拜占庭错误<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>：</strong></p> <p>出现故障（crash 或 fail-stop，即不响应）但不会伪造信息的情况称为“非拜占庭错误”。</p> <p>伪造信息恶意响应的情况称为“拜占庭错误”，对应节点称为拜占庭节点。</p></blockquote></li> <li><p>只要任何大多数（过半）服务器是可运行的，并且可以互相通信和与客户端通信，那么共识算法就可用。假设服务器宕机了，一小段时间后，它们很可能会根据已经稳定存储的状态来进行恢复，并重新加入集群。</p></li> <li><p>它们在保证日志一致性上不依赖于时序：错误的时钟和极端消息延迟在最坏的情况下会产生影响可用性的一系列问题。</p></li> <li><p>在通常情况下，只要集群中大部分（过半）服务器已经响应了单轮远程过程调用（RPC），命令就可以被视为完成。少数（一半以下）慢服务器不会影响整个系统的性能。</p></li></ul> <h2 id="_3-paxos-存在的问题"><a href="#_3-paxos-存在的问题" class="header-anchor">#</a> 3. Paxos 存在的问题</h2> <p>在过去的十年间，Leslie Lamport 的 Paxos 协议 [15] 几乎成为共识性（consensus）的同义词。它是课堂上被教授最多的共识协议，大多数共识性的实现也是以它为起点。Paxos 首先定义了能在单个决策问题（例如单个复制日志条目）上达成共识的协议。我们将这个子集称为 <em>signle-degree Paxos</em>。然后 Paxos 组合该协议的多个实例去实现一系列决策，比如日志（<em>mutil-Paxos</em>）。Paxos 保证了安全性和活性，它也支持改变集群中的成员，它的安全性也已经被论证了，并且大多数情况下都是高效的。</p> <p>美中不足的是，Paxos 有两个严重的缺点：</p> <ol><li><p><strong>Paxos 非常难理解</strong></p> <p>众所周知，Paxos 非常晦涩难懂，除非下了很大的功夫，很少有人能够成功理解它。因此，尽管目前已经有几个尝试希望将 Paxos [16,20,21]  解释得通俗易懂一些，而且这些解释都集中在 <code>single-decree Paxos</code>，但是它们还是很难懂。</p> <p>在对 NSDI 2012 参会者的非正式调查中，我们发现很少人会喜欢 Paxos，即使是经验丰富的研究人员。我们自己也一直在跟 Paxos 作斗争，我们也无法完全理解整个  Paxos 协议，直到阅读了几个更简单的描述和自己设计了替代 Paxos 的协议，我们才对 Paxos 有了比较深刻的理解。但这个过程，花了将近一年。</p> <p>我们推测 Paxos 这么晦涩难懂，主要是因为作者选择了 <code>Single-decree Paxos</code> 来作为基础。<code>Single-decree Paxso</code> 非常搞人：它分为两个阶段，但是并没有对这两个阶段进行简单直观的说明，而且这两个阶段也不能分开了单独理解，所以使用者将就很难理解为什么该算法能起作用。<code>Multi-Paxos</code> 的合成规则又增加了许多复杂性。我们相信，对多个决定（日志，并非单个日志条目）达成共识的总体问题可以用其他更直接和更明显的方式进行分解。</p></li> <li><p><strong>Paxos 没有为实际实现提供一个良好的基础</strong></p> <p>其中一个原因是没有广泛认同的针对 <code>Multi-Paxos</code> 的算法。Lamport 的描述主要是针对 <code>signle-decree Paxos</code> 的，他描述了针对 <code>multi-Paxos</code> 的可能方法，但缺少了很多细节。</p> <p>目前已经有人在尝试具体化和优化 Paxos，比如 [26]，[39] 和 [13]，但是这些尝试都互不相同并且它们跟 Lamport 描述的也不尽相同。虽然像 Chubby [4] 这样的系统已经实现了类 Paxos（Paxos-like）算法，但是他们并没有透露出很多的实现细节。</p></li></ol> <p>此外，Paxos 的架构对于构建实际系统来说其实是一个糟糕的设计，这是 <code>single-decree Paxos</code> 分解的另一个结果。举个例子，这对于独立选择地日志条目的集合，然后再将它们合并到顺序日志当中没有任何好处，这只会增加复杂性。围绕日志来设计系统是更加简单和高效的方法，其中新条目按受约束的顺序依次附加。另外一个问题是 Paxos 在其核心使用了<strong>对称对等方法</strong>（尽管它最终表明了这会被用作一种性能优化的弱领导模式）。这在只有一个决策的情况下是有意义的，但是尽管如此，还是很少有实际系统采用了这种方法。如果有一系列的决策需要制定，更简单和更快速的方法应该是首先选择一个 leader，然后由 leader 去协调这些决策。</p> <p>因此，按照 Paxos 来实现的实际系统往往跟 Paxos 相差很大。几乎所有的实现都是从 Paxos 开始，然后在实现的过程中发现了一系列的难题，在解决难题的过程中，开发出了跟 Paxos 完全不一样的架构。这样既费时又容易出错，而且 Paxos 本身的晦涩难懂又使得问题变得更加严重。Paxos 公式可能是证明其正确性的一个很好的公式，但真正的实现与 Paxos 又相差很大，这证明了它其实没有什么价值。下面来自 Chubby 作者的评论非常典型：</p> <blockquote><p>在 Paxos 算法描述和现实实现系统之间有着巨大的鸿沟... （如果一直按照 Paxos 算法走下去），最终的系统往往会建立在一个还未被证明的协议之上。</p></blockquote> <p>综合上述问题，我们觉得 Paxos 在教学端和系统构建端都没有提供一个良好的基础。考虑到共识性在大规模软件系统中的重要性，我们决定去尝试一下看看能不能设计一个替代 Paxos 并且具有更好特性的共识算法。Raft 就是这次实验的结果。</p> <h2 id="_4-为可理解性而设计"><a href="#_4-为可理解性而设计" class="header-anchor">#</a> 4. 为可理解性而设计</h2> <p>在设计 Raft 算法过程中我们有几个目标：</p> <ul><li>它必须为系统构建提供一个完整且实际的基础，这样才能大大减少开发者的工作</li> <li>它必须在任何情况下都是安全的并且在典型的应用条件下是可用的，并且在正常情况下是高效的</li></ul> <p>但是我们最重要的目标，也是我们遇到的最大的挑战：</p> <ul><li>它必须具有易理解性，它必须保证能够被大多数人轻松地理解。而且它必须能够让人形成直观的认识，这样系统构建者才能在实现过程中对它进行不可避免的拓展。</li></ul> <p>在设计 Raft 算法的过程中，很多情况下我们需要在多个备选方案下做出抉择。在这种情况下，我们往往会基于可理解性来进行抉择：</p> <ul><li>解释各个备选方案的难度有多大？例如，它的状态空间有多复杂？它是否具有难以理解的含义？</li> <li>对于一个读者来说，完成理解这个方案和方案中的各种含义是否简单？</li></ul> <p>我们意识到这一的分析具有高度的主观性。所以我们采取了两种通用的措施来解决这个问题。</p> <p>第一个措施就是众所周知的问题分解：只要有可能，我们就将问题划分成几个相对独立地解决、解释和理解的子问题。例如，Raft 算法被我们划分成 leader 选举、日志复制、安全性和成员变更几个部分。</p> <p>第二个措施是通过减少状态的数量来简化状态空间，尽可能地使系统变得更加连贯和尽可能地消除不确定性。很明显的一个例子就是，所有的日志都是不允许有空挡的，并且 Raft 限制了日志之间可能不一样的方式。尽管在大多数情况下我们都极力去消除不确定性，但是在某些情况下不确定性却可以提高可理解性。一个重要的例子就是随机化方法，它们虽然引入了不确定性，但是它们往往能够通过以类似的方式处理所有可能的选择来减少状态空间（随便选，没关系）。所有我们使用了随机化来简化 Raft 中的 leader election 算法。</p> <h2 id="_5-raft-共识算法"><a href="#_5-raft-共识算法" class="header-anchor">#</a> 5. Raft 共识算法</h2> <p>Raft 是一种用来管理第 2 节中提到的复制日志（replicated log）的算法。图 2 是该算法的浓缩，可以作为参考。图 3 列举了该算法的一些关键特性。这两张图中的内容将会在后面的各个章节中逐一介绍。</p> <p>Raft 在实现共识算法的过程中，首先选举一个 distinguished leader，然后由该 leader 全权负责复制日志的一致性。Leader 从客户端接收日志条目，然后将这些日志条目复制给其他服务器，并且在保证安全性的情况下通知其他服务器将日志条目应用到他们的状态机中。拥有一个 leader 大大简化了对复制日志的管理流程。例如，leader 可以在不跟其他服务器商议的情况下决定新的日志条目应该存放在日志的什么位置，并且数据都是从 leader 流向其他服务器。当然了，一个 leader 可能会宕机，也可能与其他服务器断开连接，那么这个时候，Raft 就会选举出一个新的 leader 出来。</p> <p>通过选举一个 leader 的方式，Raft 将共识问题分解成三个独立的子问题，这些问题将会在接下来的子章节中进行讨论：</p> <ul><li><p><strong>Leader election（领导选举）</strong></p> <p>一个 leader 倒下之后，一定会有一个新的 leader 站起来。</p></li> <li><p><strong>Log replication（日志复制）</strong></p> <p>leader 必须接收来自客户端的日志条目然后复制到集群中的其他节点，并且强制其他节点的日志和自己的保持一致。</p></li> <li><p><strong>Safety（安全性）</strong></p> <p>Raft 中安全性的关键是图 3 中状态机的安全性：只要有任何服务器节点将一个特定的日志条目应用到它的状态机中，那么其他服务器节点就不能在同一个日志索引位置上存储另外一条不同的指令。第 5.4 节将会描述 Raft 如何保证这种特性，而且该解决方案在 5.2 节描述的选举机制上还增加了额外的限制。</p></li></ul> <p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gsar1v5rklj32300pc4bj.jpg" alt="image-20210709155333989"></p> <p>在展示了 Raft 共识算法后，本章节将讨论可用性的一些问题以及时序在系统中的所用。</p> <h3 id="_5-1-raft-基础"><a href="#_5-1-raft-基础" class="header-anchor">#</a> 5.1 Raft 基础</h3> <p>一个 Raft 集群中包含若干个服务器节点，<font color="green"><strong>5 个一个比较典型的数字，5 个服务器的集群可以容忍 2 个节点的失效</strong></font>。在任何一个时刻，集群中的每一个节点都只可能是以下是三种身份之一：</p> <ul><li>leader：它会处理所有来自客户端的请求（如果一个客户端和 follower 通信，follower 会将请求重定向到 leader 上）</li> <li>follower：它们被动的：它们不会发送任何请求，只是简单的响应来自 leader 和 candidate 的请求</li> <li>candidate：这是用来选举一个新的 leader 的时候出现的一种临时状态，这将在第 5.2 节中详细描述</li></ul> <p>在正常情况下，集群中只有一个 leader，然后剩下的节点都是 follower。图 4 展示了这些状态和它们之间的转换关系，这些转换关系将会在接下来进行讨论。</p> <p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gsap8d6ijjj322s0l47g5.jpg" alt="image-20210709145034498"></p> <p>如图 5 所示，Raft 将时间划分成任意长度的任期（term）。每一段任期从一次选举开始，在这个时候会有一个或者多个 candidate 尝试去成为 leader。如果某一个 candidate 赢得了选举，那么它就会在任期剩下的时间里承担一个 leader 的角色。在某些情况下，一次选举无法选出 leader，这个时候这个任期会以没有 leader 而结束。同时一个新的任期（包含一次新的选举）会很快重新开始。这是因为 Raft 会保证在任意一个任期内，至多有一个 leader。</p> <p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gsapcmhhi4j31ym0ig78s.jpg" alt="image-20210709145441879"></p> <p>集群中不同的服务器观察到的任期转换的次数也许是不同的，在某些情况下，一个节点可能没有观察到 leader 选举过程甚至是整个任期过程。</p> <p>任期在 Raft 中还扮演着一个逻辑时钟（logical clock）的角色，这使得服务器可以发现一些过期的信息，比如过时的 leader。</p> <p>每一个节点都存储着一个当前任期号（current term number），该任期号会随着时间<strong>单调递增</strong>。节点之间通信的时候会交换当前任期号，如果一个节点的当前任期号比其他节点小，那么它就将自己的任期号更新为较大的那个值。如果一个 candidate 或者 leader 发现自己的任期号过期了，它就会立刻回到 follower 状态。如果一个节点接收了一个带着过期的任期号的请求，那么它会拒绝这次请求。</p> <p>Raft 算法中服务器节点之间采用 RPC 进行通信，一般的共识算法都只需要两种类型的 RPC。</p> <ul><li><strong>RequestVote RPCs（请求投票）</strong>：由 candidate 在选举过程中发出（5.2 节中描述）</li> <li><strong>AppendEntries RPCs（追加条目）</strong>：由 leader 发出，用来做日志复制和提供心跳机制（5.3 节中描述）。</li></ul> <p>在第 7 节中为了在节点之间传输快照（snapshot）增加了第三种 RPC。当节点没有及时的收到 RPC 的响应时，会进行重试，而且节点之间都是以并行（parallel）的方式发送 RPC 请求，以此来获得最佳的性能。</p> <h3 id="_5-2-leader-election"><a href="#_5-2-leader-election" class="header-anchor">#</a> 5.2 Leader election</h3> <p>Raft 采用一种心跳机制来触发 leader 选举。当服务器启动的时候，他们都会称为 follower。一个服务器节点只要从 candidate 或者 leader 那接收到有效的 RPC 就一直保持 follower 的状态。Leader 会周期性地向所有的 follower 发起心跳来维持自己的 leader 地位，所谓心跳，就是不包含日志条目的 AppendEntries RPC。如果一个 follower 在一段时间内没有收到任何信息（这段时间我们称为<strong>选举超时 election timeout</strong>），那么它就会假定目前集群中没有一个可用的 leader，然后开启一次选举来选择一个新的 leader。</p> <p>开始进行选举的时候，一个 follower 会自增当前任期号然后切换为 candidate 状态。然后它会给自己投票，同时以并行的方式发送一个 RequestVote RPCs 给集群中的其他服务器节点（企图得到它们的投票）。一个 candidate 会一直保持当前状态直到以下的三件事之一发生（这些情况都会在下面的章节里分别讨论）：</p> <ul><li>它赢得选举，成为了 leader</li> <li>其他节点赢得了选择，那么它会变成 follower</li> <li>一段时间之后没有任何节点在选举中胜出</li></ul> <p>当一个 candidate 获取集群中过半服务器节点针对同一任期的投票时，它就赢得了这次选举并成为新的 leader。对于同一个任期，每一个服务器节点会按照**先来先服务原则（first-come-first-served）**只投给一个 candidate（在5.4 节会在投票上增加额外的限制）。这种要求获得过半投票才能成为 leader 的规则确保了最多只有一个 candidate 赢得此次选举（图 3 中的选举安全性）。只要有一个 candidate 赢得选举，它就会成为 leader。然后它就会向集群中其他节点发送心跳消息来确定自己的地位并阻止新的选举。</p> <p>一个 candidate 在等待其他节点给它投票的时候，它也有可能接收到另外一个自称为 leader 的节点给它发过来的 AppendEntries RPC。</p> <ul><li>如果这个 leader 的任期号（这个任期号会在这次 RPC 中携带着）不小于这个 candidate 的当前任期号，那么这个 candidate 就会觉得这个 leader 是合法的，然后将自己转变为 follower 状态。</li> <li>如果这个 leader 的任期号小于这个 candidate 的当前热七号，那么这个 candidate 就会拒绝这次 RPC，然后继续保持 candidate 状态。</li></ul> <p>第三种可能的结果是 candidate 既没有赢得选举也没有输。可以设想一下这么一个长裤。所有的 follower 同时变成  candidate，然后它们都将票投给自己，那这样就没有 candidate 能得到超过半数的投票了，投票无果。当这种情况发生的时候，每个 candidate 都会进行一次超时响应（time out），然后通过自增任期号来开启一轮新的选举，并启动另一轮的 RequestVote RPCs。然而，如果没有额外的措施，这种无结果的投票可能会无限重复下去。</p> <p>为了解决上述问题，Raft 采用**随机选举超时时间（randomized election timeouts）**来确保很少发生无果的投票，并且就算发生了也能很快地解决。<strong>为了防止选票一开始就被瓜分，选举超时时间是从一个固定的区间（比如，150-300ms）中随机选择。这样可以把服务器分散开来以确保在大多数情况下会只有一个服务器率先结束超时，那么这个时候，它就可以赢得选举并在其他服务器结束超时之前发送心跳</strong>（译者注：乘虚而入，不讲武德）。</p> <p>同样的机制也可以被用来解决选票被瓜分（split votes）的情况。每个 candidate 在开始一轮选举之前会重置一个随机选举超时时间，然后一直等待直到结束超时状态。这样减少了在一次投票无果后再一次投票无果的可能性。9.3 节展示了该方案能够快速地选出一个 leader。</p> <p>选举的例子可以很好地展现可理解性是如何指导我们在多种备选设计方案中做出抉择的。在一开始，我们本打算使用一种等级系统（rank system）：每一个 candidate 被赋予一个一次的等级（rank），如果一个 candidate 发现另外一个 candidate 有着更高的登记，那么它就会返回 follower 状态，这样可以使高等级的 candidate 更加容易地赢得下一轮选举。但是我们发现这种方法在可用性方面会有一些小问题：**如果等级较高的服务器宕机了，那么等级较低的服务器可能需要进入超时状态，然后重新成为一个 candidate。如果这种操作出现得太快，那么它可能会重启进程去开启一轮新的选举。**经过我们对该算法做出了多次的调整，我们最终还是认为随机重试的方法更加通俗易懂。</p> <h3 id="_5-3-log-replication"><a href="#_5-3-log-replication" class="header-anchor">#</a> 5.3 Log replication</h3> <p>Leader 一旦被选举出来，它就要开始为客户端的请求提供服务了。每一个客户端请求都包含一条将被复制状态机执行的命令。leader 会以一个新条目的方式将该命令追加到自己的日志中，并且以同步的方式向集群中的其他节点发起 AppendEntires RPCs，让它们复制该条目。当条目被安全地复制（何为安全复制，后面会介绍）之后，leader 会将该条目应用到自己的状态机中，状态机执行该指令，然后把执行的结果返回给客户端。如果 follower 宕机了或者运行缓慢，或者网络丢包，leader 会不断地重试 AppendEntiries RPCs（即使已经对客户端作出了响应）直到所有的 follower 都成功存储了所有的日志条目。</p> <p>日志以图 6 展示的方式组织着。每条日志条目都存储着一条状态机指令和 leader 收到该指定时的任期号。日志条目中的任期号可以用来检测多个日志副本之间是否不一致，以此来保证图 3 中的某些性质。每个日志条目还有一个整数索引值来表明它在日志中的位置。</p> <p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gsasp7ss8gj32220ssjy9.jpg" alt="image-20210709165036190"></p> <p>那么问题就来了，**leader 什么时候会觉得把日志条目应用到状态机是安全的呢？**这种日志条目被称为已提交的日志条目。Raft 保证这种已提交的日志条目都是持久化的并且最终都会被所有可用的状态机执行。**一旦创建该日志条目的 leader 将它复制到过半的节点上时（比如图 6 中的条目 7），该日志条目就会被提交。**同时，leader 日志中该日志条目之前的所有日志条目也都会被提交，包括由之前的其他 leader 创建的日志条目。5.4 节会讨论在 leader 变更之后应用该规则的一些细节，并证明这种提交的规则是安全的。</p> <h2 id="参考"><a href="#参考" class="header-anchor">#</a> 参考</h2> <p>[1]  BOLOSKY, W. J., BRADSHAW, D., HAAGENS, R. B., KUSTERS, N. P., AND LI, P. Paxos replicated state machines as the basis of a high-performance data store. In <em>Proc. NSDI’11, USENIX Conference on Networked Systems Design and Implementation</em> (2011), USENIX, pp. 141–154.</p> <p>[2]  BURROWS, M. The Chubby lock service for loosely- coupled distributed systems. In <em>Proc. OSDI’06, Sympo- sium on Operating Systems Design and Implementation</em> (2006), USENIX, pp. 335–350.</p> <p>[3]  CAMARGOS, L. J., SCHMIDT, R. M., AND PEDONE, F. Multicoordinated Paxos. In <em>Proc. PODC’07, ACM Sym- posium on Principles of Distributed Computing</em> (2007), ACM, pp. 316–317.</p> <p>[4]  CHANDRA, T. D., GRIESEMER, R., AND REDSTONE, J. Paxos made live: an engineering perspective. In <em>Proc. PODC’07, ACM Symposium on Principles of Distributed Computing</em> (2007), ACM, pp. 398–407.</p> <p>[5]  CHANG, F., DEAN, J., GHEMAWAT, S., HSIEH, W. C., WALLACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., AND GRUBER, R. E. Bigtable: a distributed storage system for structured data. In <em>Proc. OSDI’06, USENIX Symposium on Operating Systems Design and Implementation</em> (2006), USENIX, pp. 205–218.</p> <p>[6]  CORBETT, J. C., DEAN, J., EPSTEIN, M., FIKES, A., FROST, C., FURMAN, J. J., GHEMAWAT, S., GUBAREV, A., HEISER, C., HOCHSCHILD, P., HSIEH, W., KAN- THAK, S., KOGAN, E., LI, H., LLOYD, A., MELNIK, S., MWAURA, D., NAGLE, D., QUINLAN, S., RAO, R., ROLIG, L., SAITO, Y., SZYMANIAK, M., TAYLOR, C., WANG, R., AND WOODFORD, D. Spanner: Google’s globally-distributed database. In <em>Proc. OSDI’12, USENIX Conference on Operating Systems Design and Implemen- tation</em> (2012), USENIX, pp. 251–264.</p> <p>[7]  COUSINEAU, D., DOLIGEZ, D., LAMPORT, L., MERZ, S., RICKETTS, D., AND VANZETTO, H. TLA+ proofs. In <em>Proc. FM’12, Symposium on Formal Methods</em> (2012), D. Giannakopoulou and D. Me ́ry, Eds., vol. 7436 of <em>Lec- ture Notes in Computer Science</em>, Springer, pp. 147–154.</p> <p>[8]  GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In <em>Proc. SOSP’03, ACM Symposium on Operating Systems Principles</em> (2003), ACM, pp. 29–43.</p> <p>[9]  GRAY,C.,ANDCHERITON,D.Leases:Anefficientfault- tolerant mechanism for distributed file cache consistency. In <em>Proceedings of the 12th ACM Ssymposium on Operating Systems Principles</em> (1989), pp. 202–210.</p> <p>[10]  HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for concurrent objects. <em>ACM Trans- actions on Programming Languages and Systems 12</em> (July 1990), 463–492.</p> <p>[11]  HUNT, P., KONAR, M., JUNQUEIRA, F. P., AND REED, B . ZooKeeper: wait-free coordination for internet-scale systems. In <em>Proc ATC’10, USENIX Annual Technical Con- ference</em> (2010), USENIX, pp. 145–158.</p> <p>[12]  JUNQUEIRA, F. P., REED, B. C., AND SERAFINI, M. Zab: High-performance broadcast for primary-backup sys- tems. In <em>Proc. DSN’11, IEEE/IFIP Int’l Conf. on Depend- able Systems &amp; Networks</em> (2011), IEEE Computer Society, pp. 245–256.</p> <p>[13]  KIRSCH, J., AND AMIR, Y. Paxos for system builders. Tech. Rep. CNDS-2008-2, Johns Hopkins University, 2008.</p> <p>[14]  L A M P O RT, L . Time, clocks, and the ordering of events in a distributed system. <em>Commununications of the ACM 21</em>, 7 (July 1978), 558–565.</p> <p>[15]  L A M P O RT, L . The part-time parliament. <em>ACM Transac- tions on Computer Systems 16</em>, 2 (May 1998), 133–169.</p> <p>[16]  LAMPORT, L. Paxos made simple. <em>ACM SIGACT News 32</em>, 4 (Dec. 2001), 18–25.</p> <p>[17]  L A M P O RT, L . <em>Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers</em>. Addison- Wesley, 2002.</p> <p>[18]  LAMPORT, L. Generalized consensus and Paxos. Tech. Rep. MSR-TR-2005-33, Microsoft Research, 2005.</p> <p>[19] L A M P O RT, L . Fast paxos. (2006), 79–103.</p> <p>[20]  LAMPSON, B. W. How to build a highly available system using consensus. In <em>Distributed Algorithms</em>, O. Baboaglu and K. Marzullo, Eds. Springer-Verlag, 1996, pp. 1–17.</p> <p>[21]  LAMPSON, B. W. The ABCD’s of Paxos. In <em>Proc. PODC’01, ACM Symposium on Principles of Distributed Computing</em> (2001), ACM, pp. 13–13.</p> <p>[22]  LISKOV, B., AND COWLING, J. Viewstamped replica- tion revisited. Tech. Rep. MIT-CSAIL-TR-2012-021, MIT, July 2012.</p> <p>[23]  LogCabin source code. http://github.com/ logcabin/logcabin.</p> <p>[24]  LORCH, J. R., ADYA, A., BOLOSKY, W. J., CHAIKEN, R., DOUCEUR, J. R., AND HOWELL, J. The SMART way to migrate replicated stateful services. In <em>Proc. Eu- roSys’06, ACM SIGOPS/EuroSys European Conference on Computer Systems</em> (2006), ACM, pp. 103–115.</p> <p>[25]  MAO, Y., JUNQUEIRA, F. P., AND MARZULLO, K. Mencius: building efficient replicated state machines for WANs. In <em>Proc. OSDI’08, USENIX Conference on Operating Systems Design and Implementation</em> (2008), USENIX, pp. 369–384.</p> <p>[26] MAZIE` RES, D. Paxos made practical. http://www.scs.stanford.edu/ ̃dm/home/ papers/paxos.pdf, Jan. 2007.</p> <p>[27]  MORARU, I., ANDERSEN, D. G., AND KAMINSKY, M. There is more consensus in egalitarian parliaments. In <em>Proc. SOSP’13, ACM Symposium on Operating System Principles</em> (2013), ACM.</p> <p>[28]  Raft user study. http://ramcloud.stanford. edu/ ̃ongaro/userstudy/.</p> <p>[29]  OKI, B. M., AND LISKOV, B. H. Viewstamped replication: A new primary copy method to support highly-available distributed systems. In <em>Proc. PODC’88, ACM Symposium on Principles of Distributed Computing</em> (1988), ACM, pp. 8–17.</p> <p>[30]  O’NEIL, P., CHENG, E., GAWLICK, D., AND ONEIL, E. The log-structured merge-tree (LSM-tree). <em>Acta Informat- ica 33</em>, 4 (1996), 351–385.</p> <p>[31]  ONGARO, D. <em>Consensus: Bridging Theory and Practice</em>. PhD thesis, Stanford University, 2014 (work in progress).</p> <p>[32]  ONGARO, D., AND OUSTERHOUT, J. In search of an understandable consensus algorithm. In <em>Proc ATC’14, USENIX Annual Technical Conference</em> (2014), USENIX.</p> <p>[33]  OUSTERHOUT, J., AGRAWAL, P., ERICKSON, D., KOZYRAKIS, C., LEVERICH, J., MAZIE`RES, D., MI- TRA, S., NARAYANAN, A., ONGARO, D., PARULKAR, G., ROSENBLUM, M., RUMBLE, S. M., STRATMANN, E., AND STUTSMAN, R. The case for RAMCloud. <em>Com- munications of the ACM 54</em> (July 2011), 121–130.</p> <p>[34]  Raft consensus algorithm website. http://raftconsensus.github.io.</p> <p>[35]  REED, B. Personal communications, May 17, 2013.</p> <p>[36]  ROSENBLUM, M., AND OUSTERHOUT, J. K. The design and implementation of a log-structured file system. <em>ACM Trans. Comput. Syst. 10</em> (February 1992), 26–52.</p> <p>[37]  S C H N E I D E R , F. B . Implementing fault-tolerant services using the state machine approach: a tutorial. <em>ACM Com- puting Surveys 22</em>, 4 (Dec. 1990), 299–319.</p> <p>[38]  SHVACHKO, K., KUANG, H., RADIA, S., AND CHANSLER, R. The Hadoop distributed file system. In <em>Proc. MSST’10, Symposium on Mass Storage Sys- tems and Technologies</em> (2010), IEEE Computer Society, pp. 1–10.</p> <p>[39]  VAN RENESSE, R. Paxos made moderately complex. Tech. rep., Cornell University, 2012.</p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">7/9/2021, 5:07:48 PM</span></div></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/noteSite/assets/js/app.ee9f4817.js" defer></script><script src="/noteSite/assets/js/2.0f8877a4.js" defer></script><script src="/noteSite/assets/js/44.c1e4736b.js" defer></script>
  </body>
</html>
